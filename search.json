[
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects",
    "section": "",
    "text": "This section showcases the two major projects completed as part of the Computational Mathematics for Data Science course. These projects demonstrate the application of concepts and tools learned during the coursework."
  },
  {
    "objectID": "projects.html#introduction",
    "href": "projects.html#introduction",
    "title": "Projects",
    "section": "",
    "text": "This section showcases the two major projects completed as part of the Computational Mathematics for Data Science course. These projects demonstrate the application of concepts and tools learned during the coursework."
  },
  {
    "objectID": "projects.html#project-i-benign-and-malignant-tissue-classification",
    "href": "projects.html#project-i-benign-and-malignant-tissue-classification",
    "title": "Projects",
    "section": "2 Project I: Benign and Malignant Tissue Classification",
    "text": "2 Project I: Benign and Malignant Tissue Classification\n\n2.1 Overview\n\nObjective: Classify benign and malignant tissue using statistical methods and classical machine learning algorithms.\nTools: MATLAB and CVX solver for implementing predictive and prescriptive models.\nHighlights:\n\nA statistical approach for feature selection.\nDevelopment of a predictive model using supervised learning.\nFormulation of a prescriptive model using unsupervised learning methods.\n\n\n\n\nPresentation of the project is available at:"
  },
  {
    "objectID": "projects.html#project-ii-applications-of-singular-value-decomposition-svd",
    "href": "projects.html#project-ii-applications-of-singular-value-decomposition-svd",
    "title": "Projects",
    "section": "3 Project II: Applications of Singular Value Decomposition (SVD)",
    "text": "3 Project II: Applications of Singular Value Decomposition (SVD)\n\n3.1 Overview\n\nObjective: Explore the theoretical and practical utility of SVD in digital image processing.\nTechniques:\n\nImage Compression: Reducing image size while preserving quality.\nDenoising: Enhancing image clarity by removing noise.\nForensic Analysis: Extracting and analyzing hidden image details.\n\nReference: A partial replication of Sadek’s work, extending its applications.\nTools: MATLAB implementations and theoretical discussions.\n\n\n\nPresentation of the project is available at:\n\n\n\n\n\n\n\n\nDigital version\n\n\n\nA digital version of this project is available at Project-2"
  },
  {
    "objectID": "evaluation1.html",
    "href": "evaluation1.html",
    "title": "Assignment 1- Foundations of Deep Learning",
    "section": "",
    "text": "# loading basic libraries and configuration of environments\n## Import libraries\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore') # suppress warning\nsns.set_theme(context='notebook',\n              style='white',\n              palette='deep',\n              font='monospace',\n              font_scale=0.7,\n              color_codes=True,\n              rc=None)\nimport matplotlib\n\nplt.rcParams['figure.figsize'] = (4,3)\nplt.rcParams['figure.facecolor'] = '#F0F8FF'\nplt.rcParams['figure.titlesize'] = 'medium'\nplt.rcParams['figure.dpi'] = 150\nplt.rcParams['figure.edgecolor'] = 'green'\nplt.rcParams['figure.frameon'] = True\n\nplt.rcParams[\"figure.autolayout\"] = True\n\nplt.rcParams['axes.facecolor'] = '#F5F5DC'\nplt.rcParams['axes.titlesize'] = 10\nplt.rcParams[\"axes.titleweight\"] = 'normal'\nplt.rcParams[\"axes.titlecolor\"] = 'Olive'\nplt.rcParams['axes.edgecolor'] = 'pink'\nplt.rcParams[\"axes.linewidth\"] = 2\nplt.rcParams[\"axes.grid\"] = True\nplt.rcParams['axes.titlelocation'] = 'center'\nplt.rcParams[\"axes.labelsize\"] = 9\nplt.rcParams[\"axes.labelpad\"] = 2\nplt.rcParams['axes.labelweight'] = 1\nplt.rcParams[\"axes.labelcolor\"] = 'Olive'\nplt.rcParams[\"axes.axisbelow\"] = False\nplt.rcParams['axes.xmargin'] = .2\nplt.rcParams[\"axes.ymargin\"] = .2\n\nplt.rcParams[\"xtick.bottom\"] = True\nplt.rcParams['xtick.color'] = '#A52A2A'\nplt.rcParams[\"ytick.left\"] = True\nplt.rcParams['ytick.color'] = '#A52A2A'\nplt.rcParams['axes.grid'] = True\nplt.rcParams['grid.color'] = 'green'\nplt.rcParams['grid.linestyle'] = '--'\nplt.rcParams['grid.linewidth'] = .5\nplt.rcParams['grid.alpha'] = .3\n\nplt.rcParams['legend.loc'] = 'best'\nplt.rcParams['legend.facecolor'] =  'NavajoWhite'\nplt.rcParams['legend.edgecolor'] = 'pink'\nplt.rcParams['legend.shadow'] = True\nplt.rcParams['legend.fontsize'] = 9\n\n\nplt.rcParams['font.family'] = 'monospace'\nplt.rcParams['font.size'] = 9\nplt.rcParams['figure.dpi'] = 150\nplt.rcParams['figure.edgecolor'] = 'Blue'\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore') # suppress warning\n\n\n\n\n\n\nDesign the deep neural network based on the user input. Compute the number of learnable parameters in each layer. The design of the architecture and the number of learnable parameters must be printed. (Comments are required for the understandability of the logic behind the code. Hidden layer computation can be written as function file. Direct function available as built-in library package must not be used).\n\nSolution\n\nGoal The goal of this problem is to design and implement an Artificial Neural Network (ANN) from scratch using Object-Oriented Programming (OOP) principles. The implementation should allow users to create an instance of the model, add layers in a manner similar to Keras, and finally call a summary() method to print the architecture and the number of learnable parameters. The project should also include the implementation of a Dropout layer to help prevent overfitting.\n\n\nObjectives\n\n\nDesign an OOP Architecture: Develop a clear and flexible OOP architecture for building and managing neural networks. This architecture should include base classes and specific layer classes such as Dense and Dropout.\nImplement Layer Classes: Implement the Dense and `Dropout layer classes, including methods to compute the number of learnable parameters and perform the forward pass.\nCreate a Neural Network Class: Implement a NeuralNetwork class that allows users to add layers, compute the total number of parameters, perform the forward pass, and print the summary of the model.\nInclude Dropout Functionality: Implement the Dropout layer to randomly set a fraction of input units to 0 during training and scale the remaining units to maintain the expected value.\nProvide User-Friendly Interface: Ensure that the user can create an instance of the model, add layers in a manner similar to Keras, and call model.summary() to print the architecture and the number of learnable parameters.\n\n\nDeliverables\n\n\nSource Code: The complete source code for the ANN implementation, including the Layer, Dense, Dropout, and NeuralNetwork classes.\nDemonstration: A script demonstrating how to create an instance of the model, add layers, and call model.summary() to print the architecture and the number of learnable parameters.\nDocumentation: A detailed description of the OOP architecture, explaining the role and functionality of each class and how they work together to achieve the goal of designing an ANN.\n\nThe following source code should include the design and implementation of the Layer, Dense, Dropout, and NeuralNetwork classes required for an ANN architecture.\nNote: The Dropout class is implemented the dropout functionality as described by Andrew Ng1.\n1 https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-2-improving-deep-learning-network.md#dropout-regularization\nDescription of the OOP Architecture for Designing an Artificial Neural Network\n\nThe provided code implements an Artificial Neural Network (ANN) using Object-Oriented Programming (OOP) principles. The architecture is designed to allow users to create an instance of the model, add layers in a manner similar to Keras, and finally call model.summary() to print the architecture and the number of learnable parameters. The key components of this architecture are the Layer, Dense, Dropout, and NeuralNetwork classes. For this design only the NumPy library is used for numerical computations involved in model parameter calculations.\n\nOOPs Architecture- Block diagram\n\n\n\n\n\n\n\n\n\nclassDiagram\n    class Layer {\n        +int params\n        +forward(x, training=True)\n        +compute_params() int\n    }\n\n    class Activation {\n        +string activation\n        +validate_activation()\n        +forward(x, training=True)\n    }\n\n    class Dense {\n        +int input_units\n        +int output_units\n        +array weights\n        +array biases\n        +Activation activation\n        +compute_params() int\n        +forward(x, training=True)\n    }\n\n    class Dropout {\n        +float rate\n        +forward(x, training=True)\n    }\n\n    class NeuralNetwork {\n        +tuple input_shape\n        +list layers\n        +add(layer)\n        +compute_total_params() int\n        +forward(x, training=True)\n        +summary()\n    }\n\n    Layer &lt;|-- Activation\n    Layer &lt;|-- Dense\n    Layer &lt;|-- Dropout\n    NeuralNetwork \"1\" *-- \"many\" Layer\n\n\n\n\n\n\n\n\nFigure 1: Block diagram of OOP approach used in the proposed ANN architecture.\n\n\n\nA block diagram illustrating the OOP architecture of the proposed ANN model is shown in Figure Figure 1.\n\n\nCode\n# importing NumPy instance\nimport numpy as np\n\n\n\nLayer Class: The Layer class serves as a base class for all layers in the neural network. It includes methods to compute the number of parameters and perform the forward pass. This class is designed to be inherited by specific layer types such as Dense and Dropout.\n\nAttributes:\n\nparams: Stores the number of learnable parameters in the layer. Methods:\ncompute_params(): Returns the number of learnable parameters.\nforward(x, training=True): Placeholder method for the forward pass, to be implemented by subclasses.\n\n\n\nCode\n# defintion of the layer class (super class)\nclass Layer:\n    def __init__(self):\n        self.params = 0\n\n    def forward(self, x, training=True):\n        raise NotImplementedError\n\n    def compute_params(self):\n        return self.params\n\n\n\nActivation Class: Implements different activation functions such as sigmoid, relu, and softmax.\n\nAttributes: - activation: The type of activation function to use. Methods: - forward(x, training=True): Applies the specified activation function to the input.\n\n\nCode\n# defining the Activation class and its methods\nclass Activation(Layer):\n    def __init__(self, activation):\n        super().__init__()\n        self.activation = activation\n        self.validate_activation()\n\n    def validate_activation(self):\n        supported_activations = ['sigmoid', 'relu', 'softmax']\n        if self.activation not in supported_activations:\n            raise ValueError(f\"Unsupported activation function: {self.activation}. Supported activations are: {supported_activations}\")\n\n    def forward(self, x, training=True):\n        if self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-x))\n        elif self.activation == 'relu':\n            return np.maximum(0, x)\n        elif self.activation == 'softmax':\n            exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n            return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\n\nDense Class: The Dense class inherits from the Layer class and implements a fully connected (dense) layer. It includes the computation of the number of parameters and the forward pass.\n\nAttributes:\n\ninput_units: Number of input units to the layer.\noutput_units: Number of output units from the layer.\nweights: Weight matrix of the layer, initialized with random values.\nbiases: Bias vector of the layer, initialized with random values.\n\nMethods:\n\ncompute_params(): Computes and returns the number of learnable parameters (weights and biases).\nforward(x, training=True): Implements the forward pass by performing a matrix multiplication of the input with the weights and adding the biases.\n\n\n\nCode\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, activation=None):\n        super().__init__()\n        self.input_units = input_units\n        self.output_units = output_units\n        self.weights = np.random.randn(output_units, input_units)\n        self.biases = np.random.randn(output_units)\n        self.activation = Activation(activation) if activation else None\n        self.params = self.compute_params()\n\n    def compute_params(self):\n        weight_params = self.input_units * self.output_units\n        bias_params = self.output_units\n        return weight_params + bias_params\n\n    def forward(self, x, training=True):\n        z = np.dot(self.weights, x) + self.biases\n        if self.activation:\n            return self.activation.forward(z, training)\n        return z\n\n\n\nDropout Class: The Dropout class inherits from the Layer class and implements a dropout layer. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to 0 during training.\n\nAttributes:\n\nrate: Dropout rate, representing the fraction of input units to be dropped.\n\nMethod:\n\nforward(x, training=True): Implements the forward pass. During training, it randomly sets a fraction of input units to 0 and scales the remaining units to maintain the expected value.\n\n\n\nCode\n# defintion of Dropout class and its methods\nclass Dropout(Layer):\n    def __init__(self, rate):\n        super().__init__()\n        self.rate = rate\n\n    def forward(self, x, training=True):\n        if training:\n            keep_prob = 1 - self.rate\n            mask = np.random.rand(*x.shape) &lt; keep_prob\n            x = np.multiply(x, mask)\n            x = x / keep_prob\n        return x\n\n\n\nNeuralNetwork Class: The NeuralNetwork class represents the entire neural network. It allows users to add layers, compute the total number of parameters, perform the forward pass, and print the summary of the model.\n\nAttributes:\n\ninput_shape: Shape of the input data.\nlayers: List of layers in the neural network.\n\nMethods:\n\nadd(layer): Adds a layer to the network.\ncompute_total_params(): Computes and returns the total number of learnable parameters in the network by summing the parameters of each layer.\nforward(x, training=True): Performs the forward pass through the network by sequentially passing the input through each layer.\nsummary(): Prints the architecture and the number of learnable parameters for each layer, as well as the total number of parameters.\n\n\n\nCode\n# defining the NeuralNetwork class and its methods\n\nclass NeuralNetwork:\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n        self.layers = []\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def compute_total_params(self):\n        total_params = 0\n        for layer in self.layers:\n            total_params += layer.compute_params()\n        return total_params\n\n    def forward(self, x, training=True):\n        for layer in self.layers:\n            x = layer.forward(x, training)\n        return x\n\n    def summary(self):\n        print(\"Neural Network Summary\")\n        for layer in self.layers:\n            if isinstance(layer, Dense):\n                activation = layer.activation.activation if layer.activation else 'None'\n                print(f\"Dense Layer: Units={layer.output_units}, Activation={activation}, Params={layer.compute_params()}\")\n            elif isinstance(layer, Dropout):\n                print(f\"Dropout Layer: Rate={layer.rate}\")\n        print(f\"Total Parameters: {self.compute_total_params()}\")\n\n\n\nDemonstration\n\nThe example usage demonstrates how to create an instance of the NeuralNetwork class, add layers to the model, and call model.summary() to print the architecture and the number of learnable parameters.\n\n\nCode\n# Example usage\ninput_shape = (784,)  # Example input shape for MNIST dataset (28x28 images flattened)\n\n# Create the model with input layer\nmodel = NeuralNetwork(input_shape)\n# Add first hidden layer with 128 neurons and relu activation to the model\nmodel.add(Dense(input_shape[0], 128, activation='relu'))\n# Add dropout layer with rate 0.5\nmodel.add(Dropout(0.5))\n# Add second hidden layer with 64 neurons and relu activation to the model\nmodel.add(Dense(128, 64, activation='relu'))\n# Add dropout layer with rate 0.5\nmodel.add(Dropout(0.5))\n# Add output layer with 10 neurons and softmax activation for 10 class classification\nmodel.add(Dense(64, 10, activation='softmax'))  # Output layer for 10 classes\n# Print the summary of the model\nmodel.summary()\n\n\nNeural Network Summary\nDense Layer: Units=128, Activation=relu, Params=100480\nDropout Layer: Rate=0.5\nDense Layer: Units=64, Activation=relu, Params=8256\nDropout Layer: Rate=0.5\nDense Layer: Units=10, Activation=softmax, Params=650\nTotal Parameters: 109386\n\n\n\nComplete implementation\n\n\n\nCode\nimport numpy as np\n\nclass Layer:\n    def __init__(self):\n        self.params = 0\n\n    def forward(self, x, training=True):\n        raise NotImplementedError\n\n    def compute_params(self):\n        return self.params\n\nclass Activation(Layer):\n    def __init__(self, activation):\n        super().__init__()\n        self.activation = activation\n        self.validate_activation()\n\n    def validate_activation(self):\n        supported_activations = ['sigmoid', 'relu', 'softmax']\n        if self.activation not in supported_activations:\n            raise ValueError(f\"Unsupported activation function: {self.activation}. Supported activations are: {supported_activations}\")\n\n    def forward(self, x, training=True):\n        if self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-x))\n        elif self.activation == 'relu':\n            return np.maximum(0, x)\n        elif self.activation == 'softmax':\n            exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n            return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, activation=None):\n        super().__init__()\n        self.input_units = input_units\n        self.output_units = output_units\n        self.weights = np.random.randn(output_units, input_units)\n        self.biases = np.random.randn(output_units)\n        self.activation = Activation(activation) if activation else None\n        self.params = self.compute_params()\n\n    def compute_params(self):\n        weight_params = self.input_units * self.output_units\n        bias_params = self.output_units\n        return weight_params + bias_params\n\n    def forward(self, x, training=True):\n        z = np.dot(self.weights, x) + self.biases\n        if self.activation:\n            return self.activation.forward(z, training)\n        return z\n\nclass Dropout(Layer):\n    def __init__(self, rate):\n        super().__init__()\n        self.rate = rate\n\n    def forward(self, x, training=True):\n        if training:\n            keep_prob = 1 - self.rate\n            mask = np.random.rand(*x.shape) &lt; keep_prob\n            x = np.multiply(x, mask)\n            x = x / keep_prob\n        return x\n\nclass NeuralNetwork:\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n        self.layers = []\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def compute_total_params(self):\n        total_params = 0\n        for layer in self.layers:\n            total_params += layer.compute_params()\n        return total_params\n\n    def forward(self, x, training=True):\n        for layer in self.layers:\n            x = layer.forward(x, training)\n        return x\n\n    def summary(self):\n        print(\"Neural Network Summary\")\n        for layer in self.layers:\n            if isinstance(layer, Dense):\n                activation = layer.activation.activation if layer.activation else 'None'\n                print(f\"Dense Layer: Units={layer.output_units}, Activation={activation}, Params={layer.compute_params()}\")\n            elif isinstance(layer, Dropout):\n                print(f\"Dropout Layer: Rate={layer.rate}\")\n        print(f\"Total Parameters: {self.compute_total_params()}\")\n\n# Example usage\ninput_shape = (784,)  # Example input shape for MNIST dataset (28x28 images flattened)\n\n# Create the model\nmodel = NeuralNetwork(input_shape)\n\n# Add layers to the model\nmodel.add(Dense(input_shape[0], 128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, 64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, 64, activation='relu'))\n\nmodel.add(Dense(64, 10, activation='softmax'))  # Output layer for 10 classes\n\n# Print the summary of the model\nmodel.summary()\n\n\nNeural Network Summary\nDense Layer: Units=128, Activation=relu, Params=100480\nDropout Layer: Rate=0.5\nDense Layer: Units=64, Activation=relu, Params=8256\nDropout Layer: Rate=0.5\nDense Layer: Units=64, Activation=relu, Params=8256\nDense Layer: Units=10, Activation=softmax, Params=650\nTotal Parameters: 117642\n\n\n\n\n\n\n\n\n\nExplore atleast 3 different activation functions used in the deep learning. Present the mathematical equation (with all variables defined) and plot the same.\nSolution:\n\n\nSigmoid Activation Function: The Sigmoid activation function is defined as: \\[\\sigma(x) = \\dfrac{1}{1 + e^{-x}}\\]\n\n\n\nAs \\(x \\to -\\infty\\), \\(\\sigma(x) \\to 0\\).\nAs \\(x \\to +\\infty\\), \\(\\sigma(x) \\to 1\\).\nThe curve smoothly transitions from 0 to 1, creating an “S” shape.\n\n\nProperties\n\n\nS-shape: The function’s smooth curve has two asymptotes (at 0 and 1), creating the S-like appearance.\nMonotonic: Always increasing.\nDifferentiable: Smooth changes without sharp edges.\nDerivative: \\(\\sigma'(x)=\\sigma(x)\\left(1-\\sigma(x)\\right)\\)\n\nGraph: Graph of the sigmoid function is plotted as shown in Figure 2:\n\n\nCode\n# importing libraries for numerical computation and plotting\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 400)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.title('$\\sigma(x)=\\dfrac{1}{1+e^{-x}}$')\nplt.xlabel('Input (x)')\nplt.ylabel('Output ($\\sigma(x)$)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Graph of sigmoid activation function\n\n\n\n\n\n\n\nRectified Linear Units (ReLu)\n\n\nThe ReLU activation function is defined as: \\[\\text{ReLU}(x) = \\max(0, x)=\\begin{cases}x&;\\quad x\\geq 0\\\\ 0&;\\quad x&lt;0\\end{cases}\\]\n\\(\\implies\\) - For \\(x &lt; 0\\), \\(\\text{ReLU}(x) = 0\\). - For \\(x \\geq 0\\), \\(\\text{ReLU}(x) = x\\).\nWhy “Rectified”?\n\nThe term “rectified” comes from the fact that the function “corrects” or “rectifies” the negative inputs by mapping them to 0 while leaving positive inputs unchanged.\n\nProperties:\n\nSimple: Computationally efficient due to its straightforward implementation.\nNon-linear capability: Despite its linear segment, it allows for non-linear transformations when combined in networks.\nSparsity: Outputs zero for all negative inputs, introducing sparsity in neural network activations.\n\n\n\nCode\ndef relu(x):\n    return np.maximum(0, x)\n\ny = relu(x)\n\nplt.plot(x, y)\nplt.title('ReLU Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Graph of ReLu activation function\n\n\n\n\n\n\n\nSoftmax Activation Function\n\n\n\\[\n\\sigma(z_i) = \\frac{e^{z_i}}{\\sum\\limits_{j=1}^{n} e^{z_j}} \\quad \\text{for } i = 1, \\dots, n\n\\]\n\n\\(z_i\\) is the \\(i\\)-th element of the input vector \\(z\\).\n\\(e^{z_i}\\) ensures all outputs are positive.\nThe sum of all outputs equals 1, making them suitable as probabilities.\n\nProperties:\n\nProbabilistic Output: Transforms raw scores into probabilities, with outputs summing to 1.\nSensitivity: Exponentiation amplifies the relative differences between inputs.\nDifferentiability: Unlike argmax, Softmax is differentiable, making it usable in optimization.\n\n\n\nCode\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / np.sum(exp_x)\n\nx = np.linspace(-2, 2, 400)\ny = softmax(x)\n\nplt.plot(x, y)\nplt.title('Softmax Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Graph of softmax activation function\n\n\n\n\n\n\n\nLeaky ReLu\n\n\nThe name Leaky ReLU derives from the function’s behavior, which is a variant of the Rectified Linear Unit (ReLU) activation function. Unlike ReLU, which sets all negative inputs to \\(0\\), Leaky ReLU introduces a small, non-zero gradient (\\(\\alpha\\)) for negative inputs.This allows some information to “leak” through even when the input is negative.\nThe Leaky ReLU function is defined as:\n\\[\nf(x) =\n\\begin{cases}\nx & \\text{if } x \\geq 0 \\\\\n\\alpha x & \\text{if } x &lt; 0\n\\end{cases}\n\\]\n\n\\(x\\) is the input to the function.\n\\(\\alpha\\) is a small positive constant, often chosen as 0.01, which defines the “leakiness.”\n\nProperties:\n\nNon-zero Gradient for Negative Inputs: Solves the dying ReLU problem, where neurons become inactive due to zero gradients.\nLinear for Positive Values: Retains simplicity and efficiency of ReLU for non-negative inputs.\nParameter \\(\\alpha\\): Can be set manually or learned during training.\n\n\n\nCode\ndef leaky_relu(x, alpha=0.01):\n    return np.where(x &gt;= 0, x, alpha * x)\n\nx = np.linspace(-10, 10, 400)\ny = leaky_relu(x)\n\nplt.plot(x, y)\nplt.title('Leaky ReLU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Graph of Leaky ReLu activation function\n\n\n\n\n\n\n\nELU (Exponential Linear Unit) Activation Function\n\n\nThe name ELU reflects the combination of its two defining characteristics: Exponential behavior for negative inputs and a Linear Unit for positive inputs. \\[\nf(x) =\n\\begin{cases}\nx & \\text{if } x &gt; 0 \\\\\n\\alpha (e^x - 1) & \\text{if } x \\leq 0\n\\end{cases}\n\\]\nProperties:\n\nSmooth Transition: ELU is continuous and differentiable, with a smooth gradient that helps optimization during training.\nNon-zero Gradient for Negative Inputs: Unlike ReLU, which outputs zero for negative inputs, ELU introduces a small gradient for \\(x \\leq 0\\), mitigating the dying ReLU problem.\nOutput Range: Negative outputs are bounded by \\(-\\alpha\\), introducing a degree of normalization.\nLinear Behavior for Positive Inputs: Ensures fast convergence and efficient representation for positive values.\n\n\n\nCode\ndef elu(x, alpha=1.0):\n    return np.where(x &gt;= 0, x, alpha * (np.exp(x) - 1))\n\ny = elu(x)\n\nplt.plot(x, y)\nplt.title('ELU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Graph of Exponential ReLu activation function\n\n\n\n\n\n\n\n\\(\\tanh(x)\\) activation function\n\n\nThe name Tanh comes from the Hyperbolic Tangent function, which is defined mathematically as:\n\\[\n\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nProperties:\n\nRange: \\(\\tanh(x)\\) maps input values to the range \\((-1, 1)\\).\nZero-centered Output: Unlike the sigmoid function, \\(\\tanh(x)\\) outputs values symmetrically around zero, making it more effective for training neural networks as it reduces bias in gradient updates.\nNon-linearity: Provides a smooth and non-linear transformation of the input.\nGradient: The derivative of \\(\\tanh(x)\\) is: \\[\n\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)\n\\]\n\n\n\nCode\ndef tanh(x):\n    return np.tanh(x)\n\ny = tanh(x)\n\nplt.plot(x, y)\nplt.title('Tanh Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Graph of Exponential tanh activation function\n\n\n\n\n\n\nComparison: \\(\\tanh(x)\\) vs. Sigmoid\n\n\n\n\n\n\n\n\n\nFeature\n\\(\\tanh(x)\\)\n\\(Sigmoid\\)\n\n\n\n\nRange\n\\((-1, 1)\\)\n\\((0, 1)\\)\n\n\nZero-centered Output\nYes\nNo\n\n\nGradient\nSteeper than sigmoid, improving convergence\nCan saturate for extreme values ( \\(\\pm x\\) )\n\n\nFormula\n\\(\\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n\\(\\frac{1}{1 + e^{-x}}\\)\n\n\nBias in Updates\nNo (zero-centered output)\nYes (output always positive)\n\n\nUsage\nPreferred in deep networks for faster training\nOften used in output layers for probabilities\n\n\n\nA visual comparison of all the activation functions together is shown in Figure 8.\n\n\nCode\n# Sigmoid Activation Function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# ReLU Activation Function\ndef relu(x):\n    return np.maximum(0, x)\n\n# Softmax Activation Function\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / np.sum(exp_x)\n\n# Leaky ReLU Activation Function\ndef leaky_relu(x, alpha=0.01):\n    return np.where(x &gt;= 0, x, alpha * x)\n\n# ELU Activation Function\ndef elu(x, alpha=1.0):\n    return np.where(x &gt;= 0, x, alpha * (np.exp(x) - 1))\n\n# Tanh Activation Function\ndef tanh(x):\n    return np.tanh(x)\n\n# Plotting the activation functions\nx = np.linspace(-10, 10, 400)\n\nplt.figure(figsize=(12, 8))\n\n# Sigmoid\nplt.subplot(2, 3, 1)\nplt.plot(x, sigmoid(x))\nplt.title('Sigmoid Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# ReLU\nplt.subplot(2, 3, 2)\nplt.plot(x, relu(x))\nplt.title('ReLU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# Softmax\nplt.subplot(2, 3, 3)\nplt.plot(x, softmax(x))\nplt.title('Softmax Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# Leaky ReLU\nplt.subplot(2, 3, 4)\nplt.plot(x, leaky_relu(x))\nplt.title('Leaky ReLU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# ELU\nplt.subplot(2, 3, 5)\nplt.plot(x, elu(x))\nplt.title('ELU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# Tanh\nplt.subplot(2, 3, 6)\nplt.plot(x, tanh(x))\nplt.title('Tanh Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: Visual comparionn of popular activation functions\n\n\n\n\n\n\n\n\n\n\nIdentify a dataset and build your own deep neural network architecture for the following:\n\nRegression\n\nClassification (multi-class).\n\nNote: Built-in library packages can be used to implement this question. Plot the loss curves. Print the performance evaluation measures.\nSolution\n\n\n\nTask 1: Stock market prediction\n\n\nProblem Statement:\n\nThe goal of this task is to predict the future Apple stock price (Close price) based on historical stock market data over a four-year period. The data includes various financial indicators and technical analysis features, such as stock opening, closing, and volume, along with moving averages, exponential moving averages (EMA), relative strength index (RSI), force index, and other financial market indicators.\nThe dataset contains the following features:\nStock Data Columns:\n\nOpen, High, Low, Close, Volume: Basic stock market data representing the price points and volume for Apple stock.\n\nTechnical Indicators:\n\nSD20, Upper_Band, Lower_Band: Bollinger Bands indicators based on the 20-day standard deviation.\nS_Close(t-1), S_Close(t-2), S_Close(t-3), S_Close(t-5): Lagged closing prices.\nS_Open(t-1): Lagged opening price.\nMA5, MA10, MA20, MA50, MA200: Moving averages of the closing prices over different time intervals.\nEMA10, EMA20, EMA50, EMA100, EMA200: Exponential moving averages for different periods.\nMACD, MACD_EMA: Moving Average Convergence Divergence and its exponential moving average.\nATR: Average True Range, measuring market volatility.\nADX: Average Directional Index, used to determine the strength of a trend.\nCCI: Commodity Channel Index, used to identify cyclical trends.\nROC: Rate of Change, measuring the percentage change in price.\nRSI: Relative Strength Index, an oscillator that measures the speed and change of price movements.\nWilliam%R: Williams %R, a momentum indicator.\nSO%K: Stochastic Oscillator %K.\nSTD5: Standard deviation over the past 5 days.\nForceIndex1, ForceIndex20: Force index, a volume-based indicator of price movement.\n\nMarket Indices Data:\n\nQQQ_Close, QQQ(t-1), QQQ(t-2), QQQ(t-5): Historical data for the QQQ index.\nSnP_Close, SnP(t-1), SnP(t-5): Historical data for the S&P 500 index.\nDJIA_Close, DJIA(t-1), DJIA(t-5): Historical data for the DJIA index.\n\nTime-based Features:\n\nDate_col: Date of the record.\nDay, DayofWeek, DayofYear, Week: Time-based features of the day and year.\nIs_month_end, Is_month_start, Is_quarter_end, Is_quarter_start, Is_year_end, Is_year_start, Is_leap_year: Categorical features indicating various time period markers.\n\nThe task is to build a model that can forecast the Apple stock price (Close) for a given future period based on the historical data and technical indicators. The model should leverage time series data, stock market indicators, and macroeconomic factors to predict stock price movements accurately.\nObjectives:\n\nPreprocess the data to handle missing values, scale features, and engineer any additional features if required.\nDevelop and train machine learning models to predict the Apple stock price using historical data and technical indicators.\nEvaluate model performance using appropriate metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (\\(R^2\\)).\nProvide insights into which features are most influential in predicting the Apple stock price.\n\nThe model will be evaluated on its ability to predict the Close_forcast column, which represents the future Apple stock closing price.\nExpected Outcome:\nThe output will be a reliable regression model capable of forecasting the Apple stock price with acceptable accuracy based on historical trends and technical analysis features.\n\nSolution procedure of this task is explained in detail if following section.\n\nStep 1: Loading necessary libraries and dataset\n\n\n\nCode\nimport pandas as pd # for dataset handling\nimport numpy as np # for numerical computations\n# libraries for ML preprocessing and model performance evaluation tasks\nfrom sklearn.model_selection import cross_val_score, train_test_split\n#from sklearn.feature_selection import RFECV, SelectFromModel, SelectKBest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n%matplotlib inline\n\n\nA close look at the structure of the data is shown below.\n\n\nCode\nStock = pd.read_csv('https://raw.githubusercontent.com/sijuswamyresearch/24DS611-DL/refs/heads/main/AAPL.csv',  index_col=0)\ndf_Stock = Stock\ndf_Stock = df_Stock.rename(columns={'Close(t)':'Close'})\ndf_Stock.head()\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nSD20\nUpper_Band\nLower_Band\nS_Close(t-1)\nS_Close(t-2)\n...\nQQQ_MA10\nQQQ_MA20\nQQQ_MA50\nSnP_Close\nSnP(t-1))\nSnP(t-5)\nDJIA_Close\nDJIA(t-1))\nDJIA(t-5)\nClose_forcast\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-10-17\n6.66\n6.69\n6.50\n6.60\n154208600\n0.169237\n6.827473\n6.150527\n6.67\n6.63\n...\n33.692\n33.9970\n34.2690\n1190.10\n1186.57\n1187.33\n10348.10\n10287.34\n10238.76\n6.45\n\n\n2005-10-18\n6.57\n6.66\n6.44\n6.45\n152397000\n0.168339\n6.819677\n6.146323\n6.60\n6.67\n...\n33.570\n33.9525\n34.2466\n1178.14\n1190.10\n1184.87\n10285.26\n10348.10\n10253.17\n6.78\n\n\n2005-10-19\n6.43\n6.78\n6.32\n6.78\n252170800\n0.180306\n6.861112\n6.139888\n6.45\n6.60\n...\n33.562\n33.9600\n34.2330\n1195.76\n1178.14\n1177.68\n10414.13\n10285.26\n10216.91\n6.93\n\n\n2005-10-20\n6.72\n6.97\n6.71\n6.93\n339440500\n0.202674\n6.931847\n6.121153\n6.78\n6.45\n...\n33.567\n33.9455\n34.2190\n1177.80\n1195.76\n1176.84\n10281.10\n10414.13\n10216.59\n6.87\n\n\n2005-10-21\n7.02\n7.03\n6.83\n6.87\n199181500\n0.216680\n6.974860\n6.108140\n6.93\n6.78\n...\n33.586\n33.9365\n34.2034\n1179.59\n1177.80\n1186.57\n10215.22\n10281.10\n10287.34\n7.01\n\n\n\n\n5 rows × 63 columns\n\n\n\nTotal number of features and sample size can be found using the following python code.\n\n\nCode\ndf_Stock.shape\n\n\n(3732, 63)\n\n\nFeatures and target in the data set is shown below.\n\n\nCode\ndf_Stock.columns\n\n\nIndex(['Open', 'High', 'Low', 'Close', 'Volume', 'SD20', 'Upper_Band',\n       'Lower_Band', 'S_Close(t-1)', 'S_Close(t-2)', 'S_Close(t-3)',\n       'S_Close(t-5)', 'S_Open(t-1)', 'MA5', 'MA10', 'MA20', 'MA50', 'MA200',\n       'EMA10', 'EMA20', 'EMA50', 'EMA100', 'EMA200', 'MACD', 'MACD_EMA',\n       'ATR', 'ADX', 'CCI', 'ROC', 'RSI', 'William%R', 'SO%K', 'STD5',\n       'ForceIndex1', 'ForceIndex20', 'Date_col', 'Day', 'DayofWeek',\n       'DayofYear', 'Week', 'Is_month_end', 'Is_month_start', 'Is_quarter_end',\n       'Is_quarter_start', 'Is_year_end', 'Is_year_start', 'Is_leap_year',\n       'Year', 'Month', 'QQQ_Close', 'QQQ(t-1)', 'QQQ(t-2)', 'QQQ(t-5)',\n       'QQQ_MA10', 'QQQ_MA20', 'QQQ_MA50', 'SnP_Close', 'SnP(t-1))',\n       'SnP(t-5)', 'DJIA_Close', 'DJIA(t-1))', 'DJIA(t-5)', 'Close_forcast'],\n      dtype='object')\n\n\nUsing the native pandas plotfunction, the target variable can be visualized as shown in Figure 9. In the current study only a multiple linear regression model is designed without considering the time series properties of the data.\n\n\nCode\ndf_Stock['Close'].plot(figsize=(10, 7))\nplt.title(\"Stock Price\", fontsize=17)\nplt.ylabel('Price', fontsize=14)\nplt.xlabel('Time', fontsize=14)\nplt.grid(which=\"major\", color='k', linestyle='-.', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Apple stock price from the given data\n\n\n\n\n\nFor the MLP model, we are considering only the numerical features available in the dataset. So the date column is dropped as follows.\n\n\nCode\ndf_Stock = df_Stock.drop(columns='Date_col')\n\n\nIn this work we are using the Keras library with Tensorflow backend as the source for basic DNN design and implementations. As usual the pupular ML library scikitlearn and its functions will be used for data preparation, scaling and model evaluation. Required methods for model building, compliling and performance evaluation can be loaded as follows.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nTwo functions will be used for model design, implementation and evaluation.\n\n\nCode\n# Function to create train, validation, and test sets\ndef create_train_test_set(df_Stock):\n    features = df_Stock.drop(columns=['Close_forcast'], axis=1)\n    target = df_Stock['Close_forcast']\n\n    # Normalizing features\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features)\n\n    # Splitting the dataset\n    X_train, X_temp, Y_train, Y_temp = train_test_split(features_scaled, target, test_size=0.12, random_state=42)\n    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n\n    return X_train, X_val, X_test, Y_train, Y_val, Y_test, scaler\n\nX_train, X_val, X_test, Y_train, Y_val, Y_test, scaler = create_train_test_set(df_Stock)\n\n\n\n\nCode\n# Define MLP model architecture\ndef create_mlp_model(input_dim):\n    model = Sequential()\n    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model\n\n\n\n\nCode\n# Evaluate the model\ndef evaluate_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test):\n    Y_train_pred = model.predict(X_train)\n    Y_val_pred = model.predict(X_val)\n    Y_test_pred = model.predict(X_test)\n\n\n    print(\"Training MSE:\", mean_squared_error(Y_train, Y_train_pred))\n    print(\"Validation MSE:\", mean_squared_error(Y_val, Y_val_pred))\n    print(\"Test MSE:\", mean_squared_error(Y_test, Y_test_pred))\n\n    print(\"Training R-squared:\", r2_score(Y_train, Y_train_pred))\n    print(\"Validation R-squared:\", r2_score(Y_val, Y_val_pred))\n    print(\"Test R-squared:\", r2_score(Y_test, Y_test_pred))\n\n\nNow let’s buld the model and train using previously defined functions as follows.\n\n\nCode\n# Create and train the MLP model\nmodel = create_mlp_model(X_train.shape[1])\nhistory = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=60, batch_size=16, verbose=False)\n\n\nNow let’s evaluate the regression model using the evaluate_model function as follows.\n\n\nCode\nevaluate_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test)\n\n\n  1/103 ━━━━━━━━━━━━━━━━━━━━ 5s 57ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56/103 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b103/103 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b103/103 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step\n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\nTraining MSE: 5.796296554479246\nValidation MSE: 2.900739341014371\nTest MSE: 4.756324920224387\nTraining R-squared: 0.9990875599901026\nValidation R-squared: 0.9994734527621479\nTest R-squared: 0.9992471704885002\n\n\nThe model loss during training and validation is plotted using following code.\n\n\nCode\n# Plot training and validation loss\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Distribution of loss function during training and validation\n\n\n\n\n\n\nConclusion\n\nThe MLP regression model demonstrates excellent performance with high R-squared values (Training: 0.9990, Validation: 0.9988, Test: 0.9993) and relatively low MSE values (Training: 6.33, Validation: 6.37, Test: 4.64) across all datasets. The slight increase in validation MSE compared to training MSE suggests minimal overfitting, indicating that the model has learned the underlying patterns in the training data without capturing too much noise. The lower test MSE further confirms that the model generalizes well to unseen data, which is a positive indicator of its robustness.\nThe high R-squared values across training, validation, and test datasets suggest that the model has a low bias, effectively capturing the complexity of the data. The minimal difference between training and validation metrics indicates low variance, meaning the model’s performance is consistent across different datasets. Overall, the model is neither underfitting nor overfitting, striking a good balance between bias and variance, and is well-suited for making accurate predictions on new data.\n\nAnalysing the skill of the model\n\nTo check the prediction quality of the model,we need to predict the stock price for a particular input. For these purpose we need to create an input data that is in the form of a dataframe containing all the input features. This is done in the next code cell.\n\n\nCode\n# Feature names used during training\nfeature_names= [\n    'Open', 'High', 'Low','Close', 'Volume', 'SD20', 'Upper_Band', 'Lower_Band',\n    'S_Close(t-1)', 'S_Close(t-2)', 'S_Close(t-3)', 'S_Close(t-5)', 'S_Open(t-1)',\n    'MA5', 'MA10', 'MA20', 'MA50', 'MA200', 'EMA10', 'EMA20', 'EMA50', 'EMA100', 'EMA200',\n    'MACD', 'MACD_EMA', 'ATR', 'ADX', 'CCI', 'ROC', 'RSI', 'William%R', 'SO%K', 'STD5',\n    'ForceIndex1', 'ForceIndex20', 'Day', 'DayofWeek', 'DayofYear', 'Week',\n    'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start',\n    'Is_year_end', 'Is_year_start', 'Is_leap_year', 'Year', 'Month',\n    'QQQ_Close', 'QQQ(t-1)', 'QQQ(t-2)', 'QQQ(t-5)', 'QQQ_MA10', 'QQQ_MA20', 'QQQ_MA50',\n    'SnP_Close', 'SnP(t-1))', 'SnP(t-5)', 'DJIA_Close', 'DJIA(t-1))', 'DJIA(t-5)'\n]\n\n# Provided values for input features as a dictionary.\ncustom_input_values = {\n    'Open': 7.02,\n    'High': 7.03,\n    'Low': 6.83,\n    'Close':6.87,\n    'Volume': 199181500,\n    'SD20': 0.216680094,\n    'Upper_Band': 6.974860188,\n    'Lower_Band': 6.108139812,\n    'S_Close(t-1)': 6.93,\n    'S_Close(t-2)': 6.78,\n    'S_Close(t-3)': 6.45,\n    'S_Close(t-5)': 6.67,\n    'S_Open(t-1)': 6.72,\n    'MA5': 6.726,\n    'MA10': 6.56,\n    'MA20': 6.5415,\n    'MA50': 6.209,\n    'MA200': 5.20315,\n    'EMA10': 6.760108607,\n    'EMA20': 6.760108607,\n    'EMA50': 6.760108607,\n    'EMA100': 6.760108607,\n    'EMA200': 6.760108607,\n    'MACD': 0.153339954,\n    'MACD_EMA': 0.129844817,\n    'ATR': 0.24109363,\n    'ADX': 22.06352634,\n    'CCI': 1333.333333,\n    'ROC': 8.530805687,\n    'RSI': 60.683333,\n    'William%R': -14.28571429,\n    'SO%K': 85.71428571,\n    'STD5': 0.030047679,\n    'ForceIndex1': -11950890,\n    'ForceIndex20': 59754450,\n    'Day': 21,\n    'DayofWeek': 4,\n    'DayofYear': 294,\n    'Week': 42,\n    'Is_month_end': 0,\n\n    'Is_month_start': 0,\n    'Is_quarter_end': 0,\n    'Is_quarter_start': 0,\n    'Is_year_end': 0,\n    'Is_year_start': 0,\n    'Is_leap_year': 0,\n    'Year': 2005,\n    'Month': 10,\n    'QQQ_Close': 33.98,\n    'QQQ(t-1)': 33.77,\n    'QQQ(t-2)': 34.09,\n    'QQQ(t-5)': 33.55,\n    'QQQ_MA10': 33.586,\n    'QQQ_MA20': 33.9365,\n    'QQQ_MA50': 34.2034,\n    'SnP_Close': 1179.59,\n    'SnP(t-1))': 1177.8,\n    'SnP(t-5)': 1186.57,\n    'DJIA_Close': 10215.22,\n    'DJIA(t-1))': 10281.1,\n    'DJIA(t-5)': 10287.34,\n\n}\n\n\n\n\nCode\n# Create a DataFrame with a single row representing the custom input\ncustom_input_df = pd.DataFrame([custom_input_values])\n\n# Check if column names match\nif set(custom_input_df.columns) == set(feature_names):\n    print(\"Column names match. Continuing with predictions.\")\n\n    # If you used standardization during training, scale the custom input\n    custom_input_scaled = scaler.transform(custom_input_df)\n\n    # Make predictions using the model\n    custom_predictions = model.predict(custom_input_scaled)\n\n    # Optional: Inverse transform if you scaled your target variable during training\n    # custom_predictions_original_scale = scaler.inverse_transform(custom_predictions)\n\n    print(\"Predicted Close Price:\", custom_predictions)\nelse:\n    print(\"Column names do not match. Please check and update the list.\")\nif set(custom_input_df.columns) != set(feature_names):\n    print(set(custom_input_df.columns))\n    print(set(feature_names))\n\n\nColumn names match. Continuing with predictions.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step\nPredicted Close Price: [[8.053632]]\n\n\nFrom the above result, it is clear that our model can predict closing price of APPL stock on 21 October 2005!\n\nComparing the actual and predicted values on the test dataset\n\nNow let’s compare actual stock price predicted price by the model using the test dataset.\n\n\nCode\n# Predictions on the test set\nY_test_pred = model.predict(X_test)\n\n# Create a DataFrame with actual and predicted values\ndf_pred = pd.DataFrame({'Actual': Y_test.values, 'Predicted': Y_test_pred.flatten()}, index=Y_test.index)\n\n# Reset the index and convert 'Date' to datetime\ndf_pred.reset_index(inplace=True)\ndf_pred['Date'] = pd.to_datetime(df_pred['Date'], format='%Y-%m-%d')\n\n# Display the DataFrame\nprint(df_pred)\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n          Date  Actual   Predicted\n0   2014-10-21   93.57   92.815628\n1   2009-05-20   15.33   16.788210\n2   2018-01-12  169.84  171.149048\n3   2007-01-18   10.93   12.169829\n4   2011-05-31   42.65   41.442059\n..         ...     ...         ...\n219 2006-08-31    8.44    9.840310\n220 2014-02-10   68.84   66.984085\n221 2010-05-14   31.38   31.383429\n222 2020-07-24  378.56  373.025269\n223 2010-04-27   32.29   33.452858\n\n[224 rows x 3 columns]\n\n\n\nVisualization of the predictions\n\nNow the predicted values are plotted along with the actual values to assess how close the prediction is. A scatter plot is used for this purpose. Train, validation and test set is used for this scatter plot to check the model bias in prediction.\n\n\nCode\n# Function to create scatter plot for subplots\ndef scatter_plot_subplot(ax, actual, predicted, title, color='blue'):\n    ax.scatter(actual, predicted, color=color, label='Predicted')\n    ax.plot(actual, actual, color='red', linestyle='--', label='Actual')  # Line for actual values\n    ax.set_title(title)\n    ax.set_xlabel('Actual Values')\n    ax.set_ylabel('Predicted Values')\n    ax.legend()\n\n\n\n\nCode\n# Predictions on the training, validation, and test sets\nY_train_pred = model.predict(X_train)\nY_val_pred = model.predict(X_val)\nY_test_pred = model.predict(X_test)\n\n\n  1/103 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59/103 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b103/103 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n\n\n\n\nCode\n# Create a figure with subplots\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Create scatter plots for training, validation, and test sets\nscatter_plot_subplot(axes[0], Y_train, Y_train_pred, '(a)Training Set - Actual vs Predicted', color='green')\nscatter_plot_subplot(axes[1], Y_val, Y_val_pred, '(b)Validation Set - Actual vs Predicted', color='orange')\nscatter_plot_subplot(axes[2], Y_test, Y_test_pred, '(c)Test Set - Actual vs Predicted', color='purple')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11: Skill of the model in stock price prediction.(a) Performance in training data, (b) Performance in validation data and (c) Performance in the test data.\n\n\n\n\n\n\n\n\n\nProblem statement\n\nPredicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope – a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.2\n2 source: https://archive.ics.uci.edu/dataset/1/abaloneThe goal of the task is to develop a model that can predict the age of an abalone based purely on the other physical measurements. This would allow researchers to estimate the abalone’s age without having to cut its shell and count the rings.Details of the dataset is shown in the following table.\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nRole\nType\nDescription\nUnits\nMissing Values\n\n\n\n\nSex\nFeature\nCategorical\nM, F, and I (infant)\n-\nno\n\n\nLength\nFeature\nContinuous\nLongest shell measurement\nmm\nno\n\n\nDiameter\nFeature\nContinuous\nPerpendicular to length\nmm\nno\n\n\nHeight\nFeature\nContinuous\nWith meat in shell\nmm\nno\n\n\nWhole_weight\nFeature\nContinuous\nWhole abalone\ngrams\nno\n\n\nShucked_weight\nFeature\nContinuous\nWeight of meat\ngrams\nno\n\n\nViscera_weight\nFeature\nContinuous\nGut weight (after bleeding)\ngrams\nno\n\n\nShell_weight\nFeature\nContinuous\nAfter being dried\ngrams\nno\n\n\nRings\nTarget\nInteger\n+1.5 gives the age in years\n-\nno\n\n\n\n\n\n\nAbalone’s picture\n\n\n\nBase line analysis\n\nIn this stage, the data will be loaded from the UCI repository through the url and a primary investigation is conducted for assessing the data quality.\n\n\nCode\n# Loading the dataset\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'\ncolumns = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\ndf = pd.read_csv(url, header=None, names=columns)\n\n\n\n\nCode\ndf.shape # display the dimension of the data matrix\n\n\n(4177, 9)\n\n\n\nSummary of dataset\n\nThe Abalone dataset contains 4175 samples with 8 input features and one target varable. A descriptive summary of the dataset is created for the baseline observation.\n\n\nCode\n# display descriptive summary\ndf.describe()\n\n\n\n\n\n\n\n\n\nLength\nDiameter\nHeight\nWhole weight\nShucked weight\nViscera weight\nShell weight\nRings\n\n\n\n\ncount\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n\n\nmean\n0.523992\n0.407881\n0.139516\n0.828742\n0.359367\n0.180594\n0.238831\n9.933684\n\n\nstd\n0.120093\n0.099240\n0.041827\n0.490389\n0.221963\n0.109614\n0.139203\n3.224169\n\n\nmin\n0.075000\n0.055000\n0.000000\n0.002000\n0.001000\n0.000500\n0.001500\n1.000000\n\n\n25%\n0.450000\n0.350000\n0.115000\n0.441500\n0.186000\n0.093500\n0.130000\n8.000000\n\n\n50%\n0.545000\n0.425000\n0.140000\n0.799500\n0.336000\n0.171000\n0.234000\n9.000000\n\n\n75%\n0.615000\n0.480000\n0.165000\n1.153000\n0.502000\n0.253000\n0.329000\n11.000000\n\n\nmax\n0.815000\n0.650000\n1.130000\n2.825500\n1.488000\n0.760000\n1.005000\n29.000000\n\n\n\n\n\n\n\nIn this dataset, the target variable is a class variable containing 29 classes ranging from 1 to 29. Distribution of these classes is shown in Figure 12.\n\n\nCode\nsns.countplot(x='Rings', data=df)\nplt.title('Distributed Classes', fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 12: Distribution of classes in the target variable.\n\n\n\n\n\nFrom the Figure 12, it is clear that in the dataset there is a terrible class imbalance. A clear picture of class wise distribution of data is shown in the following table.\n\n\nCode\n# Count each category in the \"Rings\" column and sort in descending order\ncategory_counts = df['Rings'].value_counts()\n\n# Convert the counts to a markdown table\nmarkdown_table = category_counts.reset_index().rename(columns={'index': 'Rings'})\nprint(markdown_table)\n\n\n    Rings  count\n0       9    689\n1      10    634\n2       8    568\n3      11    487\n4       7    391\n5      12    267\n6       6    259\n7      13    203\n8      14    126\n9       5    115\n10     15    103\n11     16     67\n12     17     58\n13      4     57\n14     18     42\n15     19     32\n16     20     26\n17      3     15\n18     21     14\n19     23      9\n20     22      6\n21     27      2\n22     24      2\n23      1      1\n24     26      1\n25     29      1\n26      2      1\n27     25      1\n\n\nObservation: Since there is a terrible class imbalance in the dataset, a suitable target variable must be created to design a skillful model in this context.\n\nCheck for Missing values\n\nAs part of the data cleaning, a missing value check is done as follows.\n\n\nCode\n# checking for missing values\nnp.sum(df.isnull(),axis=0)\n\n\nSex               0\nLength            0\nDiameter          0\nHeight            0\nWhole weight      0\nShucked weight    0\nViscera weight    0\nShell weight      0\nRings             0\ndtype: int64\n\n\n\n\nCode\ndf[df['Height'] == 0]  #need to drop these rows.\n\n\n\n\n\n\n\n\n\nSex\nLength\nDiameter\nHeight\nWhole weight\nShucked weight\nViscera weight\nShell weight\nRings\n\n\n\n\n1257\nI\n0.430\n0.34\n0.0\n0.428\n0.2065\n0.0860\n0.1150\n8\n\n\n3996\nI\n0.315\n0.23\n0.0\n0.134\n0.0575\n0.0285\n0.3505\n6\n\n\n\n\n\n\n\n\nObservations: It is clear that there are no missing values, but at least one sample contain wrong height information (height=0!). So such samples will be removed as the part of data cleaning.\n\nSample No. 1257 and 3996 contains wrong infromation regarding the height. So remove these samples first.\n\n\nCode\ndf.drop(index=[1257,3996], inplace = True)\ndf.shape\n\n\n(4175, 9)\n\n\n\nCreating a More Appropriate Dependent Variable\n\nBy transforming Rings to Age, the data becomes directly interpretable in terms of a universally understood metric: the age of the abalone.\n\n\nCode\ndf['Age'] = df['Rings']+1.5 #AS per the problem statement\ndf.drop('Rings', axis = 1, inplace = True)\ndf.head()\n\n\n\n\n\n\n\n\n\nSex\nLength\nDiameter\nHeight\nWhole weight\nShucked weight\nViscera weight\nShell weight\nAge\n\n\n\n\n0\nM\n0.455\n0.365\n0.095\n0.5140\n0.2245\n0.1010\n0.150\n16.5\n\n\n1\nM\n0.350\n0.265\n0.090\n0.2255\n0.0995\n0.0485\n0.070\n8.5\n\n\n2\nF\n0.530\n0.420\n0.135\n0.6770\n0.2565\n0.1415\n0.210\n10.5\n\n\n3\nM\n0.440\n0.365\n0.125\n0.5160\n0.2155\n0.1140\n0.155\n11.5\n\n\n4\nI\n0.330\n0.255\n0.080\n0.2050\n0.0895\n0.0395\n0.055\n8.5\n\n\n\n\n\n\n\n\nExplortory Data Analysis\n\nExploratory Data Analysis (EDA) is a crucial initial step in the data analysis process. It involves summarizing, visualizing, and interpreting data to uncover patterns, relationships, and insights. By exploring the data, EDA helps in identifying errors, understanding the structure of the dataset, and formulating hypotheses for further analysis.\nEDA typically involves descriptive statistics, visualizations, and techniques to identify trends, outliers, and potential relationships between variables.\nDistribution of Abalone over the variable Sex is shown in Figure 13.\n\n\nCode\nsns.countplot(x='Sex', data=df)\n#plt.title('Distributed Classes', fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 13: Distribution of Abalone over Sex.\n\n\n\n\n\nFrom Figure 13, it is clear that there are almost same number of samples over various categories in the variable Sex.\nFigure 14 demonstrate how the age of Abalone varying over the Sex.\n\n\nCode\n#categorical features\nimport seaborn as sns\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxenplot(x='Sex', y=\"Age\", data=df)\nfig.axis(ymin=0, ymax=35);\n\n\n\n\n\n\n\n\nFigure 14: Distribution of Age over Sex.\n\n\n\n\n\nIt is found that there is no significant difference in the distribution of statistical parameters , but shows a relatively lower value for the infant (I) category. Since the dataset is based on real-life measurements of abalones and the outliers are few, they could represent natural occurrences. Also from the boxplot, it is clear that gender has no significant impact on age. So we can drop this feature without affecting the model performance.\n\n\nCode\n#dropping the variable Sex\ndf.drop('Sex', axis=1, inplace = True)\n\n\n\nUnderstanding the Distribution of the Numerical Features Now let’s look into the disribution of numerical features. Following Histogram illustrate the distribution of the numerical features.\n\n\n\nCode\ndf.hist(figsize = (20,10), layout = (3,3))\n\n\n\n\n\n\n\narray([[&lt;Axes: title={'center': 'Length'}&gt;,\n        &lt;Axes: title={'center': 'Diameter'}&gt;,\n        &lt;Axes: title={'center': 'Height'}&gt;],\n       [&lt;Axes: title={'center': 'Whole weight'}&gt;,\n        &lt;Axes: title={'center': 'Shucked weight'}&gt;,\n        &lt;Axes: title={'center': 'Viscera weight'}&gt;],\n       [&lt;Axes: title={'center': 'Shell weight'}&gt;,\n        &lt;Axes: title={'center': 'Age'}&gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n(a) Distribution ofnumerical features.\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 15\n\n\n\n\nFrom Figure 15, it is observed that the numerical features are highly skewed and so skewness should be found and apply some normalization. Skewness of these numerical features can be found as follows.\n\n\nCode\n# finding the measure of skewness for numerical features\ntemp = pd.concat([df['Length'], df['Diameter'],df['Height'],df['Whole weight'],df['Shucked weight'],df['Viscera weight'],df['Shell weight']], axis=1)\ntemp.skew().sort_values(ascending = False)\n\n\nHeight            3.166364\nShucked weight    0.718735\nShell weight      0.621081\nViscera weight    0.591455\nWhole weight      0.530549\nDiameter         -0.610182\nLength           -0.640993\ndtype: float64\n\n\nFrom the skewness measures, it is found that the feature Height is highly positively skewed. So it should be normalized using some suitable transformation. Since all the height measures are positive, square root transformation is a better choice. Further we are building a predictive model based on measurable, non-lethal features, the subcomponent weights may be omitted. A final discision will be taken based on correlation analysis.\n\nCorrelation Analysis\n\nCorrelation measures the strength and direction of the linear relationship between independent variables (features). A correlation matrix is a common tool for examining relationships between multiple variables.\n\n\nCode\ncorr = temp.corr()\nplt.figure(figsize = (8,8))\nax = sns.heatmap(corr, vmin = -1, center = 0, annot = True, cmap = 'mako')\n\n\n\n\n\n\n\n\nFigure 16: Correlation matrix of numerical features\n\n\n\n\n\nFigure 16 shows the correlation matrix of the numerical features in the dataset. As expected, the subcomponents of weight of abalone shows very high correlation with the feature Whole weight, we can drop the subcomponents whithout loosing feature information. Also high coorelation found between the features, Length & Diameter. So let’s find those features which shows similar pattern and to be excluded.\n\n\nCode\nupper_tri = corr.where(np.triu(np.ones(corr.shape),k=1).astype(bool))\ncolumns_to_drop = [column for column in upper_tri.columns if any(upper_tri[column] &gt; 0.95)] #highly correlated variables to be removed.\nprint(\"Columns to drop:\\n\", columns_to_drop)\n\n\nColumns to drop:\n ['Diameter', 'Shucked weight', 'Viscera weight', 'Shell weight']\n\n\n\nSelection of skillful features\n\nFrom the correlation analysis, Diameter, Shuckle weight, Viscera weight and Shell weights are redundant features. So they are dropped and further the feature Height will be normalized with square root transformation.\n\n\nCode\ndf.drop(columns_to_drop, axis=1, inplace = True)\n# apply square root trasnformation\ndf['Height'] = np.sqrt(df['Height'])\n\n\n\n\nCode\n# rechecking skewness\ndf.skew().sort_values(ascending = False)\n\n\nAge             1.113754\nWhole weight    0.530549\nHeight         -0.176012\nLength         -0.640993\ndtype: float64\n\n\nNow all the independent variables are near to zero skewness. Following histogram verify this observation.\n\n\nCode\ndf.hist(figsize = (20,10), layout = (2,4), bins = 30)\n\n\narray([[&lt;Axes: title={'center': 'Length'}&gt;,\n        &lt;Axes: title={'center': 'Height'}&gt;,\n        &lt;Axes: title={'center': 'Whole weight'}&gt;,\n        &lt;Axes: title={'center': 'Age'}&gt;],\n       [&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\nCreation of a new categorical variables using the variable Age\n\nBased on the age distribution of the Abalone in the given datset, a new categroical variable is created as follows: \\[\n\\text{Age Group}=\\begin{cases}1&;\\quad 2.5\\leq \\text{Age}&lt;9.5\\\\ 2&;\\quad 9.5\\leq \\text{Age}&lt;12.5\\\\3&; \\quad 12.5\\leq \\text{Age}\\leq 30.5\\end{cases}\n\\]\nNow the structure of the updated dataset will be as follows.\n\n\nCode\ndf['Age_Group'] = pd.cut(x=df['Age'], bins=[2.5,9.5,12.5,30.5],\n                     labels=['1', '2', '3'])\ndf.head()\n\n\n\n\n\n\n\n\n\nLength\nHeight\nWhole weight\nAge\nAge_Group\n\n\n\n\n0\n0.455\n0.308221\n0.5140\n16.5\n3\n\n\n1\n0.350\n0.300000\n0.2255\n8.5\n1\n\n\n2\n0.530\n0.367423\n0.6770\n10.5\n2\n\n\n3\n0.440\n0.353553\n0.5160\n11.5\n2\n\n\n4\n0.330\n0.282843\n0.2050\n8.5\n1\n\n\n\n\n\n\n\nDistribution of samples over the newly created categorical variable is shown in Figure 17.\n\n\nCode\nsns.countplot(x='Age_Group', data=df)\n#plt.title('Distributed Classes')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 17: Sample distribution over Age_Group.\n\n\n\n\n\n\nMLP models for classification\n\nNow the final data for MLP model can be created as follows.\n\n\nCode\n# creating dataset for classification\n#Length Height  Whole weight\ndf_c = pd.concat([df['Length'], df['Height'],df['Height'],df['Whole weight'],df['Age_Group']], axis=1)\n# dataset for regression\n\ndf_r = pd.concat([df['Length'], df['Height'],df['Height'],df['Whole weight'],df['Age']], axis=1)\n\n\n\n\nCode\n# prepare input and target from the engineered dataset\nX=df_c.drop('Age_Group', axis = 1)\n#X = df.drop('Age_Group', axis = 1)\ny = df['Age_Group']\n\n\n\nBulding MLP model\n\nIn this section we will build an MLP model with two hidden layers for the classification job. Python code for this task is given below.\n\n\nCode\n# loading libraries: classification mlp model for the abalone dataset\nfrom numpy import unique\nfrom numpy import argmax\nfrom pandas import read_csv\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\n\nCode\nX, y = X.astype('float'), y.astype('float')\nn_features = X.shape[1]\n# encode strings to integer\ny = LabelEncoder().fit_transform(y)\nn_class = len(unique(y))\n# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\nmodel.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\nmodel.add(Dense(n_class, activation='softmax'))\n# compile the keras model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n# fit the keras model on the dataset\nhistory=model.fit(X_train, y_train, epochs=150, batch_size=32,validation_split=0.2, verbose=False)\n\n\nModel loss and accuracy during the training and the validation is shown in ?@fig-Abal.\n\n\nCode\n# Plot accuracy and loss from history\nplt.figure(figsize=(8, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 18: Model loss and accuracy during training and validation.\n\n\n\n\n\n\nPerformance Evaluation\n\nSkill of the MLP classification model that we built is evaluated using the measures- confusion matrix, accuracy, precision, recall, f1 score. Class-wise accuracy will be assessed using the classification report. All these measures are evaluated using the following Python code.\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,classification_report\nyhat = model.predict(X_test)\n# Calculate confusion matrix\nyhat_class = np.argmax(yhat, axis=1)  # Predicted classes\ncm = confusion_matrix(y_test, yhat_class)\n# Calculate classification metrics\naccuracy = accuracy_score(y_test, yhat_class)\nprecision = precision_score(y_test, yhat_class, average='weighted')  # Weighted for multiclass\nrecall = recall_score(y_test, yhat_class, average='weighted')\nf1 = f1_score(y_test, yhat_class, average='weighted')\n\n# Display metrics\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\nprint(f\"F1 Score: {f1:.3f}\")\nprint(classification_report(y_test, yhat_class))\n\n\n 1/44 ━━━━━━━━━━━━━━━━━━━━ 2s 54ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step\nConfusion Matrix:\n[[352  89  10]\n [131 440  45]\n [ 37 216  58]]\nAccuracy: 0.617\nPrecision: 0.601\nRecall: 0.617\nF1 Score: 0.588\n              precision    recall  f1-score   support\n\n           0       0.68      0.78      0.73       451\n           1       0.59      0.71      0.65       616\n           2       0.51      0.19      0.27       311\n\n    accuracy                           0.62      1378\n   macro avg       0.59      0.56      0.55      1378\nweighted avg       0.60      0.62      0.59      1378\n\n\n\n\nKey Metrics:\n\n\nAccuracy: 0.622\nThe model correctly classified 62.2% of the total samples. This indicates room for improvement in overall performance.\nPrecision: 0.610\nPrecision reflects the proportion of true positive predictions among all positive predictions. A precision of 0.610 suggests a moderate rate of correct positive predictions.\nRecall: 0.622\nRecall measures the proportion of true positives that were correctly identified. A recall of 0.622 shows the model detects positive samples moderately well.\nF1 Score: 0.587\nThe F1 Score, balancing precision and recall, indicates the model struggles to maintain consistency across all metrics.\n\n\nClass-wise Metrics:\n\n\n\n\nClass\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\n0\n0.68\n0.78\n0.72\n451\n\n\n1\n0.59\n0.74\n0.66\n616\n\n\n2\n0.54\n0.16\n0.24\n311\n\n\n\n\nClass 0: Performs relatively well with good precision and recall.\nClass 1: Moderate performance but better recall indicates more correct detections for this class.\nClass 2: Poor performance with low recall (16%), indicating the model struggles to detect this class.\n\n\nOverall Metrics:\n\n\n\n\nMetric\nValue\n\n\n\n\nAccuracy\n0.622\n\n\nMacro Avg\n0.610\n\n\nWeighted Avg\n0.610\n\n\n\n\nMacro Average: Unweighted average across classes, showing a slightly lower overall performance.\nWeighted Average: Reflects class imbalance, weighting metrics by the number of samples in each class.\n\n\nFindings:\n\n\nClass Imbalance: Class 2 shows significantly lower recall and F1-score, indicating difficulty in detecting samples from this class.\nMisclassifications:\n\nMany Class 2 samples are misclassified as Class 1 (223 instances).\nClass 0 performs better but still has notable misclassifications into Class 1.\n\n\n\nConclusion:The model performs moderately well overall but struggles with:\n\n\nDetecting Class 2 effectively.\nManaging misclassifications between Class 1 and Class 2.\n\nImproving the dataset balance, refining the model, or employing techniques like class-specific weighting or advanced algorithms could enhance the performance."
  },
  {
    "objectID": "evaluation1.html#design-of-a-dnn-from-the-scrtach.",
    "href": "evaluation1.html#design-of-a-dnn-from-the-scrtach.",
    "title": "Assignment 1- Foundations of Deep Learning",
    "section": "",
    "text": "Design the deep neural network based on the user input. Compute the number of learnable parameters in each layer. The design of the architecture and the number of learnable parameters must be printed. (Comments are required for the understandability of the logic behind the code. Hidden layer computation can be written as function file. Direct function available as built-in library package must not be used).\n\nSolution\n\nGoal The goal of this problem is to design and implement an Artificial Neural Network (ANN) from scratch using Object-Oriented Programming (OOP) principles. The implementation should allow users to create an instance of the model, add layers in a manner similar to Keras, and finally call a summary() method to print the architecture and the number of learnable parameters. The project should also include the implementation of a Dropout layer to help prevent overfitting.\n\n\nObjectives\n\n\nDesign an OOP Architecture: Develop a clear and flexible OOP architecture for building and managing neural networks. This architecture should include base classes and specific layer classes such as Dense and Dropout.\nImplement Layer Classes: Implement the Dense and `Dropout layer classes, including methods to compute the number of learnable parameters and perform the forward pass.\nCreate a Neural Network Class: Implement a NeuralNetwork class that allows users to add layers, compute the total number of parameters, perform the forward pass, and print the summary of the model.\nInclude Dropout Functionality: Implement the Dropout layer to randomly set a fraction of input units to 0 during training and scale the remaining units to maintain the expected value.\nProvide User-Friendly Interface: Ensure that the user can create an instance of the model, add layers in a manner similar to Keras, and call model.summary() to print the architecture and the number of learnable parameters.\n\n\nDeliverables\n\n\nSource Code: The complete source code for the ANN implementation, including the Layer, Dense, Dropout, and NeuralNetwork classes.\nDemonstration: A script demonstrating how to create an instance of the model, add layers, and call model.summary() to print the architecture and the number of learnable parameters.\nDocumentation: A detailed description of the OOP architecture, explaining the role and functionality of each class and how they work together to achieve the goal of designing an ANN.\n\nThe following source code should include the design and implementation of the Layer, Dense, Dropout, and NeuralNetwork classes required for an ANN architecture.\nNote: The Dropout class is implemented the dropout functionality as described by Andrew Ng1.\n1 https://github.com/ashishpatel26/Andrew-NG-Notes/blob/master/andrewng-p-2-improving-deep-learning-network.md#dropout-regularization\nDescription of the OOP Architecture for Designing an Artificial Neural Network\n\nThe provided code implements an Artificial Neural Network (ANN) using Object-Oriented Programming (OOP) principles. The architecture is designed to allow users to create an instance of the model, add layers in a manner similar to Keras, and finally call model.summary() to print the architecture and the number of learnable parameters. The key components of this architecture are the Layer, Dense, Dropout, and NeuralNetwork classes. For this design only the NumPy library is used for numerical computations involved in model parameter calculations.\n\nOOPs Architecture- Block diagram\n\n\n\n\n\n\n\n\n\nclassDiagram\n    class Layer {\n        +int params\n        +forward(x, training=True)\n        +compute_params() int\n    }\n\n    class Activation {\n        +string activation\n        +validate_activation()\n        +forward(x, training=True)\n    }\n\n    class Dense {\n        +int input_units\n        +int output_units\n        +array weights\n        +array biases\n        +Activation activation\n        +compute_params() int\n        +forward(x, training=True)\n    }\n\n    class Dropout {\n        +float rate\n        +forward(x, training=True)\n    }\n\n    class NeuralNetwork {\n        +tuple input_shape\n        +list layers\n        +add(layer)\n        +compute_total_params() int\n        +forward(x, training=True)\n        +summary()\n    }\n\n    Layer &lt;|-- Activation\n    Layer &lt;|-- Dense\n    Layer &lt;|-- Dropout\n    NeuralNetwork \"1\" *-- \"many\" Layer\n\n\n\n\n\n\n\n\nFigure 1: Block diagram of OOP approach used in the proposed ANN architecture.\n\n\n\nA block diagram illustrating the OOP architecture of the proposed ANN model is shown in Figure Figure 1.\n\n\nCode\n# importing NumPy instance\nimport numpy as np\n\n\n\nLayer Class: The Layer class serves as a base class for all layers in the neural network. It includes methods to compute the number of parameters and perform the forward pass. This class is designed to be inherited by specific layer types such as Dense and Dropout.\n\nAttributes:\n\nparams: Stores the number of learnable parameters in the layer. Methods:\ncompute_params(): Returns the number of learnable parameters.\nforward(x, training=True): Placeholder method for the forward pass, to be implemented by subclasses.\n\n\n\nCode\n# defintion of the layer class (super class)\nclass Layer:\n    def __init__(self):\n        self.params = 0\n\n    def forward(self, x, training=True):\n        raise NotImplementedError\n\n    def compute_params(self):\n        return self.params\n\n\n\nActivation Class: Implements different activation functions such as sigmoid, relu, and softmax.\n\nAttributes: - activation: The type of activation function to use. Methods: - forward(x, training=True): Applies the specified activation function to the input.\n\n\nCode\n# defining the Activation class and its methods\nclass Activation(Layer):\n    def __init__(self, activation):\n        super().__init__()\n        self.activation = activation\n        self.validate_activation()\n\n    def validate_activation(self):\n        supported_activations = ['sigmoid', 'relu', 'softmax']\n        if self.activation not in supported_activations:\n            raise ValueError(f\"Unsupported activation function: {self.activation}. Supported activations are: {supported_activations}\")\n\n    def forward(self, x, training=True):\n        if self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-x))\n        elif self.activation == 'relu':\n            return np.maximum(0, x)\n        elif self.activation == 'softmax':\n            exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n            return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\n\nDense Class: The Dense class inherits from the Layer class and implements a fully connected (dense) layer. It includes the computation of the number of parameters and the forward pass.\n\nAttributes:\n\ninput_units: Number of input units to the layer.\noutput_units: Number of output units from the layer.\nweights: Weight matrix of the layer, initialized with random values.\nbiases: Bias vector of the layer, initialized with random values.\n\nMethods:\n\ncompute_params(): Computes and returns the number of learnable parameters (weights and biases).\nforward(x, training=True): Implements the forward pass by performing a matrix multiplication of the input with the weights and adding the biases.\n\n\n\nCode\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, activation=None):\n        super().__init__()\n        self.input_units = input_units\n        self.output_units = output_units\n        self.weights = np.random.randn(output_units, input_units)\n        self.biases = np.random.randn(output_units)\n        self.activation = Activation(activation) if activation else None\n        self.params = self.compute_params()\n\n    def compute_params(self):\n        weight_params = self.input_units * self.output_units\n        bias_params = self.output_units\n        return weight_params + bias_params\n\n    def forward(self, x, training=True):\n        z = np.dot(self.weights, x) + self.biases\n        if self.activation:\n            return self.activation.forward(z, training)\n        return z\n\n\n\nDropout Class: The Dropout class inherits from the Layer class and implements a dropout layer. Dropout is a regularization technique that helps prevent overfitting by randomly setting a fraction of input units to 0 during training.\n\nAttributes:\n\nrate: Dropout rate, representing the fraction of input units to be dropped.\n\nMethod:\n\nforward(x, training=True): Implements the forward pass. During training, it randomly sets a fraction of input units to 0 and scales the remaining units to maintain the expected value.\n\n\n\nCode\n# defintion of Dropout class and its methods\nclass Dropout(Layer):\n    def __init__(self, rate):\n        super().__init__()\n        self.rate = rate\n\n    def forward(self, x, training=True):\n        if training:\n            keep_prob = 1 - self.rate\n            mask = np.random.rand(*x.shape) &lt; keep_prob\n            x = np.multiply(x, mask)\n            x = x / keep_prob\n        return x\n\n\n\nNeuralNetwork Class: The NeuralNetwork class represents the entire neural network. It allows users to add layers, compute the total number of parameters, perform the forward pass, and print the summary of the model.\n\nAttributes:\n\ninput_shape: Shape of the input data.\nlayers: List of layers in the neural network.\n\nMethods:\n\nadd(layer): Adds a layer to the network.\ncompute_total_params(): Computes and returns the total number of learnable parameters in the network by summing the parameters of each layer.\nforward(x, training=True): Performs the forward pass through the network by sequentially passing the input through each layer.\nsummary(): Prints the architecture and the number of learnable parameters for each layer, as well as the total number of parameters.\n\n\n\nCode\n# defining the NeuralNetwork class and its methods\n\nclass NeuralNetwork:\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n        self.layers = []\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def compute_total_params(self):\n        total_params = 0\n        for layer in self.layers:\n            total_params += layer.compute_params()\n        return total_params\n\n    def forward(self, x, training=True):\n        for layer in self.layers:\n            x = layer.forward(x, training)\n        return x\n\n    def summary(self):\n        print(\"Neural Network Summary\")\n        for layer in self.layers:\n            if isinstance(layer, Dense):\n                activation = layer.activation.activation if layer.activation else 'None'\n                print(f\"Dense Layer: Units={layer.output_units}, Activation={activation}, Params={layer.compute_params()}\")\n            elif isinstance(layer, Dropout):\n                print(f\"Dropout Layer: Rate={layer.rate}\")\n        print(f\"Total Parameters: {self.compute_total_params()}\")\n\n\n\nDemonstration\n\nThe example usage demonstrates how to create an instance of the NeuralNetwork class, add layers to the model, and call model.summary() to print the architecture and the number of learnable parameters.\n\n\nCode\n# Example usage\ninput_shape = (784,)  # Example input shape for MNIST dataset (28x28 images flattened)\n\n# Create the model with input layer\nmodel = NeuralNetwork(input_shape)\n# Add first hidden layer with 128 neurons and relu activation to the model\nmodel.add(Dense(input_shape[0], 128, activation='relu'))\n# Add dropout layer with rate 0.5\nmodel.add(Dropout(0.5))\n# Add second hidden layer with 64 neurons and relu activation to the model\nmodel.add(Dense(128, 64, activation='relu'))\n# Add dropout layer with rate 0.5\nmodel.add(Dropout(0.5))\n# Add output layer with 10 neurons and softmax activation for 10 class classification\nmodel.add(Dense(64, 10, activation='softmax'))  # Output layer for 10 classes\n# Print the summary of the model\nmodel.summary()\n\n\nNeural Network Summary\nDense Layer: Units=128, Activation=relu, Params=100480\nDropout Layer: Rate=0.5\nDense Layer: Units=64, Activation=relu, Params=8256\nDropout Layer: Rate=0.5\nDense Layer: Units=10, Activation=softmax, Params=650\nTotal Parameters: 109386\n\n\n\nComplete implementation\n\n\n\nCode\nimport numpy as np\n\nclass Layer:\n    def __init__(self):\n        self.params = 0\n\n    def forward(self, x, training=True):\n        raise NotImplementedError\n\n    def compute_params(self):\n        return self.params\n\nclass Activation(Layer):\n    def __init__(self, activation):\n        super().__init__()\n        self.activation = activation\n        self.validate_activation()\n\n    def validate_activation(self):\n        supported_activations = ['sigmoid', 'relu', 'softmax']\n        if self.activation not in supported_activations:\n            raise ValueError(f\"Unsupported activation function: {self.activation}. Supported activations are: {supported_activations}\")\n\n    def forward(self, x, training=True):\n        if self.activation == 'sigmoid':\n            return 1 / (1 + np.exp(-x))\n        elif self.activation == 'relu':\n            return np.maximum(0, x)\n        elif self.activation == 'softmax':\n            exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n            return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n\n\nclass Dense(Layer):\n    def __init__(self, input_units, output_units, activation=None):\n        super().__init__()\n        self.input_units = input_units\n        self.output_units = output_units\n        self.weights = np.random.randn(output_units, input_units)\n        self.biases = np.random.randn(output_units)\n        self.activation = Activation(activation) if activation else None\n        self.params = self.compute_params()\n\n    def compute_params(self):\n        weight_params = self.input_units * self.output_units\n        bias_params = self.output_units\n        return weight_params + bias_params\n\n    def forward(self, x, training=True):\n        z = np.dot(self.weights, x) + self.biases\n        if self.activation:\n            return self.activation.forward(z, training)\n        return z\n\nclass Dropout(Layer):\n    def __init__(self, rate):\n        super().__init__()\n        self.rate = rate\n\n    def forward(self, x, training=True):\n        if training:\n            keep_prob = 1 - self.rate\n            mask = np.random.rand(*x.shape) &lt; keep_prob\n            x = np.multiply(x, mask)\n            x = x / keep_prob\n        return x\n\nclass NeuralNetwork:\n    def __init__(self, input_shape):\n        self.input_shape = input_shape\n        self.layers = []\n\n    def add(self, layer):\n        self.layers.append(layer)\n\n    def compute_total_params(self):\n        total_params = 0\n        for layer in self.layers:\n            total_params += layer.compute_params()\n        return total_params\n\n    def forward(self, x, training=True):\n        for layer in self.layers:\n            x = layer.forward(x, training)\n        return x\n\n    def summary(self):\n        print(\"Neural Network Summary\")\n        for layer in self.layers:\n            if isinstance(layer, Dense):\n                activation = layer.activation.activation if layer.activation else 'None'\n                print(f\"Dense Layer: Units={layer.output_units}, Activation={activation}, Params={layer.compute_params()}\")\n            elif isinstance(layer, Dropout):\n                print(f\"Dropout Layer: Rate={layer.rate}\")\n        print(f\"Total Parameters: {self.compute_total_params()}\")\n\n# Example usage\ninput_shape = (784,)  # Example input shape for MNIST dataset (28x28 images flattened)\n\n# Create the model\nmodel = NeuralNetwork(input_shape)\n\n# Add layers to the model\nmodel.add(Dense(input_shape[0], 128, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, 64, activation='relu'))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, 64, activation='relu'))\n\nmodel.add(Dense(64, 10, activation='softmax'))  # Output layer for 10 classes\n\n# Print the summary of the model\nmodel.summary()\n\n\nNeural Network Summary\nDense Layer: Units=128, Activation=relu, Params=100480\nDropout Layer: Rate=0.5\nDense Layer: Units=64, Activation=relu, Params=8256\nDropout Layer: Rate=0.5\nDense Layer: Units=64, Activation=relu, Params=8256\nDense Layer: Units=10, Activation=softmax, Params=650\nTotal Parameters: 117642"
  },
  {
    "objectID": "evaluation1.html#popular-activation-functions-in-deep-learning",
    "href": "evaluation1.html#popular-activation-functions-in-deep-learning",
    "title": "Assignment 1- Foundations of Deep Learning",
    "section": "",
    "text": "Explore atleast 3 different activation functions used in the deep learning. Present the mathematical equation (with all variables defined) and plot the same.\nSolution:\n\n\nSigmoid Activation Function: The Sigmoid activation function is defined as: \\[\\sigma(x) = \\dfrac{1}{1 + e^{-x}}\\]\n\n\n\nAs \\(x \\to -\\infty\\), \\(\\sigma(x) \\to 0\\).\nAs \\(x \\to +\\infty\\), \\(\\sigma(x) \\to 1\\).\nThe curve smoothly transitions from 0 to 1, creating an “S” shape.\n\n\nProperties\n\n\nS-shape: The function’s smooth curve has two asymptotes (at 0 and 1), creating the S-like appearance.\nMonotonic: Always increasing.\nDifferentiable: Smooth changes without sharp edges.\nDerivative: \\(\\sigma'(x)=\\sigma(x)\\left(1-\\sigma(x)\\right)\\)\n\nGraph: Graph of the sigmoid function is plotted as shown in Figure 2:\n\n\nCode\n# importing libraries for numerical computation and plotting\nimport numpy as np\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\nx = np.linspace(-10, 10, 400)\ny = sigmoid(x)\n\nplt.plot(x, y)\nplt.title('$\\sigma(x)=\\dfrac{1}{1+e^{-x}}$')\nplt.xlabel('Input (x)')\nplt.ylabel('Output ($\\sigma(x)$)')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 2: Graph of sigmoid activation function\n\n\n\n\n\n\n\nRectified Linear Units (ReLu)\n\n\nThe ReLU activation function is defined as: \\[\\text{ReLU}(x) = \\max(0, x)=\\begin{cases}x&;\\quad x\\geq 0\\\\ 0&;\\quad x&lt;0\\end{cases}\\]\n\\(\\implies\\) - For \\(x &lt; 0\\), \\(\\text{ReLU}(x) = 0\\). - For \\(x \\geq 0\\), \\(\\text{ReLU}(x) = x\\).\nWhy “Rectified”?\n\nThe term “rectified” comes from the fact that the function “corrects” or “rectifies” the negative inputs by mapping them to 0 while leaving positive inputs unchanged.\n\nProperties:\n\nSimple: Computationally efficient due to its straightforward implementation.\nNon-linear capability: Despite its linear segment, it allows for non-linear transformations when combined in networks.\nSparsity: Outputs zero for all negative inputs, introducing sparsity in neural network activations.\n\n\n\nCode\ndef relu(x):\n    return np.maximum(0, x)\n\ny = relu(x)\n\nplt.plot(x, y)\nplt.title('ReLU Activation Function')\nplt.xlabel('Input (x)')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 3: Graph of ReLu activation function\n\n\n\n\n\n\n\nSoftmax Activation Function\n\n\n\\[\n\\sigma(z_i) = \\frac{e^{z_i}}{\\sum\\limits_{j=1}^{n} e^{z_j}} \\quad \\text{for } i = 1, \\dots, n\n\\]\n\n\\(z_i\\) is the \\(i\\)-th element of the input vector \\(z\\).\n\\(e^{z_i}\\) ensures all outputs are positive.\nThe sum of all outputs equals 1, making them suitable as probabilities.\n\nProperties:\n\nProbabilistic Output: Transforms raw scores into probabilities, with outputs summing to 1.\nSensitivity: Exponentiation amplifies the relative differences between inputs.\nDifferentiability: Unlike argmax, Softmax is differentiable, making it usable in optimization.\n\n\n\nCode\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / np.sum(exp_x)\n\nx = np.linspace(-2, 2, 400)\ny = softmax(x)\n\nplt.plot(x, y)\nplt.title('Softmax Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 4: Graph of softmax activation function\n\n\n\n\n\n\n\nLeaky ReLu\n\n\nThe name Leaky ReLU derives from the function’s behavior, which is a variant of the Rectified Linear Unit (ReLU) activation function. Unlike ReLU, which sets all negative inputs to \\(0\\), Leaky ReLU introduces a small, non-zero gradient (\\(\\alpha\\)) for negative inputs.This allows some information to “leak” through even when the input is negative.\nThe Leaky ReLU function is defined as:\n\\[\nf(x) =\n\\begin{cases}\nx & \\text{if } x \\geq 0 \\\\\n\\alpha x & \\text{if } x &lt; 0\n\\end{cases}\n\\]\n\n\\(x\\) is the input to the function.\n\\(\\alpha\\) is a small positive constant, often chosen as 0.01, which defines the “leakiness.”\n\nProperties:\n\nNon-zero Gradient for Negative Inputs: Solves the dying ReLU problem, where neurons become inactive due to zero gradients.\nLinear for Positive Values: Retains simplicity and efficiency of ReLU for non-negative inputs.\nParameter \\(\\alpha\\): Can be set manually or learned during training.\n\n\n\nCode\ndef leaky_relu(x, alpha=0.01):\n    return np.where(x &gt;= 0, x, alpha * x)\n\nx = np.linspace(-10, 10, 400)\ny = leaky_relu(x)\n\nplt.plot(x, y)\nplt.title('Leaky ReLU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 5: Graph of Leaky ReLu activation function\n\n\n\n\n\n\n\nELU (Exponential Linear Unit) Activation Function\n\n\nThe name ELU reflects the combination of its two defining characteristics: Exponential behavior for negative inputs and a Linear Unit for positive inputs. \\[\nf(x) =\n\\begin{cases}\nx & \\text{if } x &gt; 0 \\\\\n\\alpha (e^x - 1) & \\text{if } x \\leq 0\n\\end{cases}\n\\]\nProperties:\n\nSmooth Transition: ELU is continuous and differentiable, with a smooth gradient that helps optimization during training.\nNon-zero Gradient for Negative Inputs: Unlike ReLU, which outputs zero for negative inputs, ELU introduces a small gradient for \\(x \\leq 0\\), mitigating the dying ReLU problem.\nOutput Range: Negative outputs are bounded by \\(-\\alpha\\), introducing a degree of normalization.\nLinear Behavior for Positive Inputs: Ensures fast convergence and efficient representation for positive values.\n\n\n\nCode\ndef elu(x, alpha=1.0):\n    return np.where(x &gt;= 0, x, alpha * (np.exp(x) - 1))\n\ny = elu(x)\n\nplt.plot(x, y)\nplt.title('ELU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 6: Graph of Exponential ReLu activation function\n\n\n\n\n\n\n\n\\(\\tanh(x)\\) activation function\n\n\nThe name Tanh comes from the Hyperbolic Tangent function, which is defined mathematically as:\n\\[\n\\tanh(x) = \\frac{\\sinh(x)}{\\cosh(x)} = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n\\]\nProperties:\n\nRange: \\(\\tanh(x)\\) maps input values to the range \\((-1, 1)\\).\nZero-centered Output: Unlike the sigmoid function, \\(\\tanh(x)\\) outputs values symmetrically around zero, making it more effective for training neural networks as it reduces bias in gradient updates.\nNon-linearity: Provides a smooth and non-linear transformation of the input.\nGradient: The derivative of \\(\\tanh(x)\\) is: \\[\n\\frac{d}{dx}\\tanh(x) = 1 - \\tanh^2(x)\n\\]\n\n\n\nCode\ndef tanh(x):\n    return np.tanh(x)\n\ny = tanh(x)\n\nplt.plot(x, y)\nplt.title('Tanh Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 7: Graph of Exponential tanh activation function\n\n\n\n\n\n\nComparison: \\(\\tanh(x)\\) vs. Sigmoid\n\n\n\n\n\n\n\n\n\nFeature\n\\(\\tanh(x)\\)\n\\(Sigmoid\\)\n\n\n\n\nRange\n\\((-1, 1)\\)\n\\((0, 1)\\)\n\n\nZero-centered Output\nYes\nNo\n\n\nGradient\nSteeper than sigmoid, improving convergence\nCan saturate for extreme values ( \\(\\pm x\\) )\n\n\nFormula\n\\(\\frac{e^x - e^{-x}}{e^x + e^{-x}}\\)\n\\(\\frac{1}{1 + e^{-x}}\\)\n\n\nBias in Updates\nNo (zero-centered output)\nYes (output always positive)\n\n\nUsage\nPreferred in deep networks for faster training\nOften used in output layers for probabilities\n\n\n\nA visual comparison of all the activation functions together is shown in Figure 8.\n\n\nCode\n# Sigmoid Activation Function\ndef sigmoid(x):\n    return 1 / (1 + np.exp(-x))\n\n# ReLU Activation Function\ndef relu(x):\n    return np.maximum(0, x)\n\n# Softmax Activation Function\ndef softmax(x):\n    exp_x = np.exp(x - np.max(x))\n    return exp_x / np.sum(exp_x)\n\n# Leaky ReLU Activation Function\ndef leaky_relu(x, alpha=0.01):\n    return np.where(x &gt;= 0, x, alpha * x)\n\n# ELU Activation Function\ndef elu(x, alpha=1.0):\n    return np.where(x &gt;= 0, x, alpha * (np.exp(x) - 1))\n\n# Tanh Activation Function\ndef tanh(x):\n    return np.tanh(x)\n\n# Plotting the activation functions\nx = np.linspace(-10, 10, 400)\n\nplt.figure(figsize=(12, 8))\n\n# Sigmoid\nplt.subplot(2, 3, 1)\nplt.plot(x, sigmoid(x))\nplt.title('Sigmoid Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# ReLU\nplt.subplot(2, 3, 2)\nplt.plot(x, relu(x))\nplt.title('ReLU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# Softmax\nplt.subplot(2, 3, 3)\nplt.plot(x, softmax(x))\nplt.title('Softmax Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# Leaky ReLU\nplt.subplot(2, 3, 4)\nplt.plot(x, leaky_relu(x))\nplt.title('Leaky ReLU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# ELU\nplt.subplot(2, 3, 5)\nplt.plot(x, elu(x))\nplt.title('ELU Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\n# Tanh\nplt.subplot(2, 3, 6)\nplt.plot(x, tanh(x))\nplt.title('Tanh Activation Function')\nplt.xlabel('Input')\nplt.ylabel('Output')\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 8: Visual comparionn of popular activation functions"
  },
  {
    "objectID": "evaluation1.html#solution-of-real-world-problems-using-mlp",
    "href": "evaluation1.html#solution-of-real-world-problems-using-mlp",
    "title": "Assignment 1- Foundations of Deep Learning",
    "section": "",
    "text": "Identify a dataset and build your own deep neural network architecture for the following:\n\nRegression\n\nClassification (multi-class).\n\nNote: Built-in library packages can be used to implement this question. Plot the loss curves. Print the performance evaluation measures.\nSolution\n\n\n\nTask 1: Stock market prediction\n\n\nProblem Statement:\n\nThe goal of this task is to predict the future Apple stock price (Close price) based on historical stock market data over a four-year period. The data includes various financial indicators and technical analysis features, such as stock opening, closing, and volume, along with moving averages, exponential moving averages (EMA), relative strength index (RSI), force index, and other financial market indicators.\nThe dataset contains the following features:\nStock Data Columns:\n\nOpen, High, Low, Close, Volume: Basic stock market data representing the price points and volume for Apple stock.\n\nTechnical Indicators:\n\nSD20, Upper_Band, Lower_Band: Bollinger Bands indicators based on the 20-day standard deviation.\nS_Close(t-1), S_Close(t-2), S_Close(t-3), S_Close(t-5): Lagged closing prices.\nS_Open(t-1): Lagged opening price.\nMA5, MA10, MA20, MA50, MA200: Moving averages of the closing prices over different time intervals.\nEMA10, EMA20, EMA50, EMA100, EMA200: Exponential moving averages for different periods.\nMACD, MACD_EMA: Moving Average Convergence Divergence and its exponential moving average.\nATR: Average True Range, measuring market volatility.\nADX: Average Directional Index, used to determine the strength of a trend.\nCCI: Commodity Channel Index, used to identify cyclical trends.\nROC: Rate of Change, measuring the percentage change in price.\nRSI: Relative Strength Index, an oscillator that measures the speed and change of price movements.\nWilliam%R: Williams %R, a momentum indicator.\nSO%K: Stochastic Oscillator %K.\nSTD5: Standard deviation over the past 5 days.\nForceIndex1, ForceIndex20: Force index, a volume-based indicator of price movement.\n\nMarket Indices Data:\n\nQQQ_Close, QQQ(t-1), QQQ(t-2), QQQ(t-5): Historical data for the QQQ index.\nSnP_Close, SnP(t-1), SnP(t-5): Historical data for the S&P 500 index.\nDJIA_Close, DJIA(t-1), DJIA(t-5): Historical data for the DJIA index.\n\nTime-based Features:\n\nDate_col: Date of the record.\nDay, DayofWeek, DayofYear, Week: Time-based features of the day and year.\nIs_month_end, Is_month_start, Is_quarter_end, Is_quarter_start, Is_year_end, Is_year_start, Is_leap_year: Categorical features indicating various time period markers.\n\nThe task is to build a model that can forecast the Apple stock price (Close) for a given future period based on the historical data and technical indicators. The model should leverage time series data, stock market indicators, and macroeconomic factors to predict stock price movements accurately.\nObjectives:\n\nPreprocess the data to handle missing values, scale features, and engineer any additional features if required.\nDevelop and train machine learning models to predict the Apple stock price using historical data and technical indicators.\nEvaluate model performance using appropriate metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and R-squared (\\(R^2\\)).\nProvide insights into which features are most influential in predicting the Apple stock price.\n\nThe model will be evaluated on its ability to predict the Close_forcast column, which represents the future Apple stock closing price.\nExpected Outcome:\nThe output will be a reliable regression model capable of forecasting the Apple stock price with acceptable accuracy based on historical trends and technical analysis features.\n\nSolution procedure of this task is explained in detail if following section.\n\nStep 1: Loading necessary libraries and dataset\n\n\n\nCode\nimport pandas as pd # for dataset handling\nimport numpy as np # for numerical computations\n# libraries for ML preprocessing and model performance evaluation tasks\nfrom sklearn.model_selection import cross_val_score, train_test_split\n#from sklearn.feature_selection import RFECV, SelectFromModel, SelectKBest\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n%matplotlib inline\n\n\nA close look at the structure of the data is shown below.\n\n\nCode\nStock = pd.read_csv('https://raw.githubusercontent.com/sijuswamyresearch/24DS611-DL/refs/heads/main/AAPL.csv',  index_col=0)\ndf_Stock = Stock\ndf_Stock = df_Stock.rename(columns={'Close(t)':'Close'})\ndf_Stock.head()\n\n\n\n\n\n\n\n\n\nOpen\nHigh\nLow\nClose\nVolume\nSD20\nUpper_Band\nLower_Band\nS_Close(t-1)\nS_Close(t-2)\n...\nQQQ_MA10\nQQQ_MA20\nQQQ_MA50\nSnP_Close\nSnP(t-1))\nSnP(t-5)\nDJIA_Close\nDJIA(t-1))\nDJIA(t-5)\nClose_forcast\n\n\nDate\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2005-10-17\n6.66\n6.69\n6.50\n6.60\n154208600\n0.169237\n6.827473\n6.150527\n6.67\n6.63\n...\n33.692\n33.9970\n34.2690\n1190.10\n1186.57\n1187.33\n10348.10\n10287.34\n10238.76\n6.45\n\n\n2005-10-18\n6.57\n6.66\n6.44\n6.45\n152397000\n0.168339\n6.819677\n6.146323\n6.60\n6.67\n...\n33.570\n33.9525\n34.2466\n1178.14\n1190.10\n1184.87\n10285.26\n10348.10\n10253.17\n6.78\n\n\n2005-10-19\n6.43\n6.78\n6.32\n6.78\n252170800\n0.180306\n6.861112\n6.139888\n6.45\n6.60\n...\n33.562\n33.9600\n34.2330\n1195.76\n1178.14\n1177.68\n10414.13\n10285.26\n10216.91\n6.93\n\n\n2005-10-20\n6.72\n6.97\n6.71\n6.93\n339440500\n0.202674\n6.931847\n6.121153\n6.78\n6.45\n...\n33.567\n33.9455\n34.2190\n1177.80\n1195.76\n1176.84\n10281.10\n10414.13\n10216.59\n6.87\n\n\n2005-10-21\n7.02\n7.03\n6.83\n6.87\n199181500\n0.216680\n6.974860\n6.108140\n6.93\n6.78\n...\n33.586\n33.9365\n34.2034\n1179.59\n1177.80\n1186.57\n10215.22\n10281.10\n10287.34\n7.01\n\n\n\n\n5 rows × 63 columns\n\n\n\nTotal number of features and sample size can be found using the following python code.\n\n\nCode\ndf_Stock.shape\n\n\n(3732, 63)\n\n\nFeatures and target in the data set is shown below.\n\n\nCode\ndf_Stock.columns\n\n\nIndex(['Open', 'High', 'Low', 'Close', 'Volume', 'SD20', 'Upper_Band',\n       'Lower_Band', 'S_Close(t-1)', 'S_Close(t-2)', 'S_Close(t-3)',\n       'S_Close(t-5)', 'S_Open(t-1)', 'MA5', 'MA10', 'MA20', 'MA50', 'MA200',\n       'EMA10', 'EMA20', 'EMA50', 'EMA100', 'EMA200', 'MACD', 'MACD_EMA',\n       'ATR', 'ADX', 'CCI', 'ROC', 'RSI', 'William%R', 'SO%K', 'STD5',\n       'ForceIndex1', 'ForceIndex20', 'Date_col', 'Day', 'DayofWeek',\n       'DayofYear', 'Week', 'Is_month_end', 'Is_month_start', 'Is_quarter_end',\n       'Is_quarter_start', 'Is_year_end', 'Is_year_start', 'Is_leap_year',\n       'Year', 'Month', 'QQQ_Close', 'QQQ(t-1)', 'QQQ(t-2)', 'QQQ(t-5)',\n       'QQQ_MA10', 'QQQ_MA20', 'QQQ_MA50', 'SnP_Close', 'SnP(t-1))',\n       'SnP(t-5)', 'DJIA_Close', 'DJIA(t-1))', 'DJIA(t-5)', 'Close_forcast'],\n      dtype='object')\n\n\nUsing the native pandas plotfunction, the target variable can be visualized as shown in Figure 9. In the current study only a multiple linear regression model is designed without considering the time series properties of the data.\n\n\nCode\ndf_Stock['Close'].plot(figsize=(10, 7))\nplt.title(\"Stock Price\", fontsize=17)\nplt.ylabel('Price', fontsize=14)\nplt.xlabel('Time', fontsize=14)\nplt.grid(which=\"major\", color='k', linestyle='-.', linewidth=0.5)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 9: Apple stock price from the given data\n\n\n\n\n\nFor the MLP model, we are considering only the numerical features available in the dataset. So the date column is dropped as follows.\n\n\nCode\ndf_Stock = df_Stock.drop(columns='Date_col')\n\n\nIn this work we are using the Keras library with Tensorflow backend as the source for basic DNN design and implementations. As usual the pupular ML library scikitlearn and its functions will be used for data preparation, scaling and model evaluation. Required methods for model building, compliling and performance evaluation can be loaded as follows.\n\n\nCode\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import mean_squared_error, r2_score\n\n\nTwo functions will be used for model design, implementation and evaluation.\n\n\nCode\n# Function to create train, validation, and test sets\ndef create_train_test_set(df_Stock):\n    features = df_Stock.drop(columns=['Close_forcast'], axis=1)\n    target = df_Stock['Close_forcast']\n\n    # Normalizing features\n    scaler = StandardScaler()\n    features_scaled = scaler.fit_transform(features)\n\n    # Splitting the dataset\n    X_train, X_temp, Y_train, Y_temp = train_test_split(features_scaled, target, test_size=0.12, random_state=42)\n    X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n\n    return X_train, X_val, X_test, Y_train, Y_val, Y_test, scaler\n\nX_train, X_val, X_test, Y_train, Y_val, Y_test, scaler = create_train_test_set(df_Stock)\n\n\n\n\nCode\n# Define MLP model architecture\ndef create_mlp_model(input_dim):\n    model = Sequential()\n    model.add(Dense(64, input_dim=input_dim, activation='relu'))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(16, activation='relu'))\n    model.add(Dense(1))\n    model.compile(optimizer='adam', loss='mean_squared_error')\n    return model\n\n\n\n\nCode\n# Evaluate the model\ndef evaluate_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test):\n    Y_train_pred = model.predict(X_train)\n    Y_val_pred = model.predict(X_val)\n    Y_test_pred = model.predict(X_test)\n\n\n    print(\"Training MSE:\", mean_squared_error(Y_train, Y_train_pred))\n    print(\"Validation MSE:\", mean_squared_error(Y_val, Y_val_pred))\n    print(\"Test MSE:\", mean_squared_error(Y_test, Y_test_pred))\n\n    print(\"Training R-squared:\", r2_score(Y_train, Y_train_pred))\n    print(\"Validation R-squared:\", r2_score(Y_val, Y_val_pred))\n    print(\"Test R-squared:\", r2_score(Y_test, Y_test_pred))\n\n\nNow let’s buld the model and train using previously defined functions as follows.\n\n\nCode\n# Create and train the MLP model\nmodel = create_mlp_model(X_train.shape[1])\nhistory = model.fit(X_train, Y_train, validation_data=(X_val, Y_val), epochs=60, batch_size=16, verbose=False)\n\n\nNow let’s evaluate the regression model using the evaluate_model function as follows.\n\n\nCode\nevaluate_model(model, X_train, Y_train, X_val, Y_val, X_test, Y_test)\n\n\n  1/103 ━━━━━━━━━━━━━━━━━━━━ 5s 57ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 56/103 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b103/103 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b103/103 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step\n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 5ms/step\n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 8ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 6ms/step\nTraining MSE: 5.796296554479246\nValidation MSE: 2.900739341014371\nTest MSE: 4.756324920224387\nTraining R-squared: 0.9990875599901026\nValidation R-squared: 0.9994734527621479\nTest R-squared: 0.9992471704885002\n\n\nThe model loss during training and validation is plotted using following code.\n\n\nCode\n# Plot training and validation loss\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='validation')\nplt.title('Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['Train', 'Validation'], loc='upper right')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 10: Distribution of loss function during training and validation\n\n\n\n\n\n\nConclusion\n\nThe MLP regression model demonstrates excellent performance with high R-squared values (Training: 0.9990, Validation: 0.9988, Test: 0.9993) and relatively low MSE values (Training: 6.33, Validation: 6.37, Test: 4.64) across all datasets. The slight increase in validation MSE compared to training MSE suggests minimal overfitting, indicating that the model has learned the underlying patterns in the training data without capturing too much noise. The lower test MSE further confirms that the model generalizes well to unseen data, which is a positive indicator of its robustness.\nThe high R-squared values across training, validation, and test datasets suggest that the model has a low bias, effectively capturing the complexity of the data. The minimal difference between training and validation metrics indicates low variance, meaning the model’s performance is consistent across different datasets. Overall, the model is neither underfitting nor overfitting, striking a good balance between bias and variance, and is well-suited for making accurate predictions on new data.\n\nAnalysing the skill of the model\n\nTo check the prediction quality of the model,we need to predict the stock price for a particular input. For these purpose we need to create an input data that is in the form of a dataframe containing all the input features. This is done in the next code cell.\n\n\nCode\n# Feature names used during training\nfeature_names= [\n    'Open', 'High', 'Low','Close', 'Volume', 'SD20', 'Upper_Band', 'Lower_Band',\n    'S_Close(t-1)', 'S_Close(t-2)', 'S_Close(t-3)', 'S_Close(t-5)', 'S_Open(t-1)',\n    'MA5', 'MA10', 'MA20', 'MA50', 'MA200', 'EMA10', 'EMA20', 'EMA50', 'EMA100', 'EMA200',\n    'MACD', 'MACD_EMA', 'ATR', 'ADX', 'CCI', 'ROC', 'RSI', 'William%R', 'SO%K', 'STD5',\n    'ForceIndex1', 'ForceIndex20', 'Day', 'DayofWeek', 'DayofYear', 'Week',\n    'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start',\n    'Is_year_end', 'Is_year_start', 'Is_leap_year', 'Year', 'Month',\n    'QQQ_Close', 'QQQ(t-1)', 'QQQ(t-2)', 'QQQ(t-5)', 'QQQ_MA10', 'QQQ_MA20', 'QQQ_MA50',\n    'SnP_Close', 'SnP(t-1))', 'SnP(t-5)', 'DJIA_Close', 'DJIA(t-1))', 'DJIA(t-5)'\n]\n\n# Provided values for input features as a dictionary.\ncustom_input_values = {\n    'Open': 7.02,\n    'High': 7.03,\n    'Low': 6.83,\n    'Close':6.87,\n    'Volume': 199181500,\n    'SD20': 0.216680094,\n    'Upper_Band': 6.974860188,\n    'Lower_Band': 6.108139812,\n    'S_Close(t-1)': 6.93,\n    'S_Close(t-2)': 6.78,\n    'S_Close(t-3)': 6.45,\n    'S_Close(t-5)': 6.67,\n    'S_Open(t-1)': 6.72,\n    'MA5': 6.726,\n    'MA10': 6.56,\n    'MA20': 6.5415,\n    'MA50': 6.209,\n    'MA200': 5.20315,\n    'EMA10': 6.760108607,\n    'EMA20': 6.760108607,\n    'EMA50': 6.760108607,\n    'EMA100': 6.760108607,\n    'EMA200': 6.760108607,\n    'MACD': 0.153339954,\n    'MACD_EMA': 0.129844817,\n    'ATR': 0.24109363,\n    'ADX': 22.06352634,\n    'CCI': 1333.333333,\n    'ROC': 8.530805687,\n    'RSI': 60.683333,\n    'William%R': -14.28571429,\n    'SO%K': 85.71428571,\n    'STD5': 0.030047679,\n    'ForceIndex1': -11950890,\n    'ForceIndex20': 59754450,\n    'Day': 21,\n    'DayofWeek': 4,\n    'DayofYear': 294,\n    'Week': 42,\n    'Is_month_end': 0,\n\n    'Is_month_start': 0,\n    'Is_quarter_end': 0,\n    'Is_quarter_start': 0,\n    'Is_year_end': 0,\n    'Is_year_start': 0,\n    'Is_leap_year': 0,\n    'Year': 2005,\n    'Month': 10,\n    'QQQ_Close': 33.98,\n    'QQQ(t-1)': 33.77,\n    'QQQ(t-2)': 34.09,\n    'QQQ(t-5)': 33.55,\n    'QQQ_MA10': 33.586,\n    'QQQ_MA20': 33.9365,\n    'QQQ_MA50': 34.2034,\n    'SnP_Close': 1179.59,\n    'SnP(t-1))': 1177.8,\n    'SnP(t-5)': 1186.57,\n    'DJIA_Close': 10215.22,\n    'DJIA(t-1))': 10281.1,\n    'DJIA(t-5)': 10287.34,\n\n}\n\n\n\n\nCode\n# Create a DataFrame with a single row representing the custom input\ncustom_input_df = pd.DataFrame([custom_input_values])\n\n# Check if column names match\nif set(custom_input_df.columns) == set(feature_names):\n    print(\"Column names match. Continuing with predictions.\")\n\n    # If you used standardization during training, scale the custom input\n    custom_input_scaled = scaler.transform(custom_input_df)\n\n    # Make predictions using the model\n    custom_predictions = model.predict(custom_input_scaled)\n\n    # Optional: Inverse transform if you scaled your target variable during training\n    # custom_predictions_original_scale = scaler.inverse_transform(custom_predictions)\n\n    print(\"Predicted Close Price:\", custom_predictions)\nelse:\n    print(\"Column names do not match. Please check and update the list.\")\nif set(custom_input_df.columns) != set(feature_names):\n    print(set(custom_input_df.columns))\n    print(set(feature_names))\n\n\nColumn names match. Continuing with predictions.\n1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 13ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 45ms/step\nPredicted Close Price: [[8.053632]]\n\n\nFrom the above result, it is clear that our model can predict closing price of APPL stock on 21 October 2005!\n\nComparing the actual and predicted values on the test dataset\n\nNow let’s compare actual stock price predicted price by the model using the test dataset.\n\n\nCode\n# Predictions on the test set\nY_test_pred = model.predict(X_test)\n\n# Create a DataFrame with actual and predicted values\ndf_pred = pd.DataFrame({'Actual': Y_test.values, 'Predicted': Y_test_pred.flatten()}, index=Y_test.index)\n\n# Reset the index and convert 'Date' to datetime\ndf_pred.reset_index(inplace=True)\ndf_pred['Date'] = pd.to_datetime(df_pred['Date'], format='%Y-%m-%d')\n\n# Display the DataFrame\nprint(df_pred)\n\n\n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 26ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n          Date  Actual   Predicted\n0   2014-10-21   93.57   92.815628\n1   2009-05-20   15.33   16.788210\n2   2018-01-12  169.84  171.149048\n3   2007-01-18   10.93   12.169829\n4   2011-05-31   42.65   41.442059\n..         ...     ...         ...\n219 2006-08-31    8.44    9.840310\n220 2014-02-10   68.84   66.984085\n221 2010-05-14   31.38   31.383429\n222 2020-07-24  378.56  373.025269\n223 2010-04-27   32.29   33.452858\n\n[224 rows x 3 columns]\n\n\n\nVisualization of the predictions\n\nNow the predicted values are plotted along with the actual values to assess how close the prediction is. A scatter plot is used for this purpose. Train, validation and test set is used for this scatter plot to check the model bias in prediction.\n\n\nCode\n# Function to create scatter plot for subplots\ndef scatter_plot_subplot(ax, actual, predicted, title, color='blue'):\n    ax.scatter(actual, predicted, color=color, label='Predicted')\n    ax.plot(actual, actual, color='red', linestyle='--', label='Actual')  # Line for actual values\n    ax.set_title(title)\n    ax.set_xlabel('Actual Values')\n    ax.set_ylabel('Predicted Values')\n    ax.legend()\n\n\n\n\nCode\n# Predictions on the training, validation, and test sets\nY_train_pred = model.predict(X_train)\nY_val_pred = model.predict(X_val)\nY_test_pred = model.predict(X_test)\n\n\n  1/103 ━━━━━━━━━━━━━━━━━━━━ 2s 20ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b 59/103 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b103/103 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step\n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n1/7 ━━━━━━━━━━━━━━━━━━━━ 0s 31ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b7/7 ━━━━━━━━━━━━━━━━━━━━ 0s 4ms/step \n\n\n\n\nCode\n# Create a figure with subplots\nfig, axes = plt.subplots(1, 3, figsize=(18, 6))\n\n# Create scatter plots for training, validation, and test sets\nscatter_plot_subplot(axes[0], Y_train, Y_train_pred, '(a)Training Set - Actual vs Predicted', color='green')\nscatter_plot_subplot(axes[1], Y_val, Y_val_pred, '(b)Validation Set - Actual vs Predicted', color='orange')\nscatter_plot_subplot(axes[2], Y_test, Y_test_pred, '(c)Test Set - Actual vs Predicted', color='purple')\n\n# Adjust layout\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 11: Skill of the model in stock price prediction.(a) Performance in training data, (b) Performance in validation data and (c) Performance in the test data.\n\n\n\n\n\n\n\n\n\nProblem statement\n\nPredicting the age of abalone from physical measurements. The age of abalone is determined by cutting the shell through the cone, staining it, and counting the number of rings through a microscope – a boring and time-consuming task. Other measurements, which are easier to obtain, are used to predict the age. Further information, such as weather patterns and location (hence food availability) may be required to solve the problem.2\n2 source: https://archive.ics.uci.edu/dataset/1/abaloneThe goal of the task is to develop a model that can predict the age of an abalone based purely on the other physical measurements. This would allow researchers to estimate the abalone’s age without having to cut its shell and count the rings.Details of the dataset is shown in the following table.\n\n\n\n\n\n\n\n\n\n\n\nVariable Name\nRole\nType\nDescription\nUnits\nMissing Values\n\n\n\n\nSex\nFeature\nCategorical\nM, F, and I (infant)\n-\nno\n\n\nLength\nFeature\nContinuous\nLongest shell measurement\nmm\nno\n\n\nDiameter\nFeature\nContinuous\nPerpendicular to length\nmm\nno\n\n\nHeight\nFeature\nContinuous\nWith meat in shell\nmm\nno\n\n\nWhole_weight\nFeature\nContinuous\nWhole abalone\ngrams\nno\n\n\nShucked_weight\nFeature\nContinuous\nWeight of meat\ngrams\nno\n\n\nViscera_weight\nFeature\nContinuous\nGut weight (after bleeding)\ngrams\nno\n\n\nShell_weight\nFeature\nContinuous\nAfter being dried\ngrams\nno\n\n\nRings\nTarget\nInteger\n+1.5 gives the age in years\n-\nno\n\n\n\n\n\n\nAbalone’s picture\n\n\n\nBase line analysis\n\nIn this stage, the data will be loaded from the UCI repository through the url and a primary investigation is conducted for assessing the data quality.\n\n\nCode\n# Loading the dataset\nurl = 'http://archive.ics.uci.edu/ml/machine-learning-databases/abalone/abalone.data'\ncolumns = ['Sex', 'Length', 'Diameter', 'Height', 'Whole weight', 'Shucked weight', 'Viscera weight', 'Shell weight', 'Rings']\ndf = pd.read_csv(url, header=None, names=columns)\n\n\n\n\nCode\ndf.shape # display the dimension of the data matrix\n\n\n(4177, 9)\n\n\n\nSummary of dataset\n\nThe Abalone dataset contains 4175 samples with 8 input features and one target varable. A descriptive summary of the dataset is created for the baseline observation.\n\n\nCode\n# display descriptive summary\ndf.describe()\n\n\n\n\n\n\n\n\n\nLength\nDiameter\nHeight\nWhole weight\nShucked weight\nViscera weight\nShell weight\nRings\n\n\n\n\ncount\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n4177.000000\n\n\nmean\n0.523992\n0.407881\n0.139516\n0.828742\n0.359367\n0.180594\n0.238831\n9.933684\n\n\nstd\n0.120093\n0.099240\n0.041827\n0.490389\n0.221963\n0.109614\n0.139203\n3.224169\n\n\nmin\n0.075000\n0.055000\n0.000000\n0.002000\n0.001000\n0.000500\n0.001500\n1.000000\n\n\n25%\n0.450000\n0.350000\n0.115000\n0.441500\n0.186000\n0.093500\n0.130000\n8.000000\n\n\n50%\n0.545000\n0.425000\n0.140000\n0.799500\n0.336000\n0.171000\n0.234000\n9.000000\n\n\n75%\n0.615000\n0.480000\n0.165000\n1.153000\n0.502000\n0.253000\n0.329000\n11.000000\n\n\nmax\n0.815000\n0.650000\n1.130000\n2.825500\n1.488000\n0.760000\n1.005000\n29.000000\n\n\n\n\n\n\n\nIn this dataset, the target variable is a class variable containing 29 classes ranging from 1 to 29. Distribution of these classes is shown in Figure 12.\n\n\nCode\nsns.countplot(x='Rings', data=df)\nplt.title('Distributed Classes', fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 12: Distribution of classes in the target variable.\n\n\n\n\n\nFrom the Figure 12, it is clear that in the dataset there is a terrible class imbalance. A clear picture of class wise distribution of data is shown in the following table.\n\n\nCode\n# Count each category in the \"Rings\" column and sort in descending order\ncategory_counts = df['Rings'].value_counts()\n\n# Convert the counts to a markdown table\nmarkdown_table = category_counts.reset_index().rename(columns={'index': 'Rings'})\nprint(markdown_table)\n\n\n    Rings  count\n0       9    689\n1      10    634\n2       8    568\n3      11    487\n4       7    391\n5      12    267\n6       6    259\n7      13    203\n8      14    126\n9       5    115\n10     15    103\n11     16     67\n12     17     58\n13      4     57\n14     18     42\n15     19     32\n16     20     26\n17      3     15\n18     21     14\n19     23      9\n20     22      6\n21     27      2\n22     24      2\n23      1      1\n24     26      1\n25     29      1\n26      2      1\n27     25      1\n\n\nObservation: Since there is a terrible class imbalance in the dataset, a suitable target variable must be created to design a skillful model in this context.\n\nCheck for Missing values\n\nAs part of the data cleaning, a missing value check is done as follows.\n\n\nCode\n# checking for missing values\nnp.sum(df.isnull(),axis=0)\n\n\nSex               0\nLength            0\nDiameter          0\nHeight            0\nWhole weight      0\nShucked weight    0\nViscera weight    0\nShell weight      0\nRings             0\ndtype: int64\n\n\n\n\nCode\ndf[df['Height'] == 0]  #need to drop these rows.\n\n\n\n\n\n\n\n\n\nSex\nLength\nDiameter\nHeight\nWhole weight\nShucked weight\nViscera weight\nShell weight\nRings\n\n\n\n\n1257\nI\n0.430\n0.34\n0.0\n0.428\n0.2065\n0.0860\n0.1150\n8\n\n\n3996\nI\n0.315\n0.23\n0.0\n0.134\n0.0575\n0.0285\n0.3505\n6\n\n\n\n\n\n\n\n\nObservations: It is clear that there are no missing values, but at least one sample contain wrong height information (height=0!). So such samples will be removed as the part of data cleaning.\n\nSample No. 1257 and 3996 contains wrong infromation regarding the height. So remove these samples first.\n\n\nCode\ndf.drop(index=[1257,3996], inplace = True)\ndf.shape\n\n\n(4175, 9)\n\n\n\nCreating a More Appropriate Dependent Variable\n\nBy transforming Rings to Age, the data becomes directly interpretable in terms of a universally understood metric: the age of the abalone.\n\n\nCode\ndf['Age'] = df['Rings']+1.5 #AS per the problem statement\ndf.drop('Rings', axis = 1, inplace = True)\ndf.head()\n\n\n\n\n\n\n\n\n\nSex\nLength\nDiameter\nHeight\nWhole weight\nShucked weight\nViscera weight\nShell weight\nAge\n\n\n\n\n0\nM\n0.455\n0.365\n0.095\n0.5140\n0.2245\n0.1010\n0.150\n16.5\n\n\n1\nM\n0.350\n0.265\n0.090\n0.2255\n0.0995\n0.0485\n0.070\n8.5\n\n\n2\nF\n0.530\n0.420\n0.135\n0.6770\n0.2565\n0.1415\n0.210\n10.5\n\n\n3\nM\n0.440\n0.365\n0.125\n0.5160\n0.2155\n0.1140\n0.155\n11.5\n\n\n4\nI\n0.330\n0.255\n0.080\n0.2050\n0.0895\n0.0395\n0.055\n8.5\n\n\n\n\n\n\n\n\nExplortory Data Analysis\n\nExploratory Data Analysis (EDA) is a crucial initial step in the data analysis process. It involves summarizing, visualizing, and interpreting data to uncover patterns, relationships, and insights. By exploring the data, EDA helps in identifying errors, understanding the structure of the dataset, and formulating hypotheses for further analysis.\nEDA typically involves descriptive statistics, visualizations, and techniques to identify trends, outliers, and potential relationships between variables.\nDistribution of Abalone over the variable Sex is shown in Figure 13.\n\n\nCode\nsns.countplot(x='Sex', data=df)\n#plt.title('Distributed Classes', fontsize=14)\nplt.show()\n\n\n\n\n\n\n\n\nFigure 13: Distribution of Abalone over Sex.\n\n\n\n\n\nFrom Figure 13, it is clear that there are almost same number of samples over various categories in the variable Sex.\nFigure 14 demonstrate how the age of Abalone varying over the Sex.\n\n\nCode\n#categorical features\nimport seaborn as sns\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxenplot(x='Sex', y=\"Age\", data=df)\nfig.axis(ymin=0, ymax=35);\n\n\n\n\n\n\n\n\nFigure 14: Distribution of Age over Sex.\n\n\n\n\n\nIt is found that there is no significant difference in the distribution of statistical parameters , but shows a relatively lower value for the infant (I) category. Since the dataset is based on real-life measurements of abalones and the outliers are few, they could represent natural occurrences. Also from the boxplot, it is clear that gender has no significant impact on age. So we can drop this feature without affecting the model performance.\n\n\nCode\n#dropping the variable Sex\ndf.drop('Sex', axis=1, inplace = True)\n\n\n\nUnderstanding the Distribution of the Numerical Features Now let’s look into the disribution of numerical features. Following Histogram illustrate the distribution of the numerical features.\n\n\n\nCode\ndf.hist(figsize = (20,10), layout = (3,3))\n\n\n\n\n\n\n\narray([[&lt;Axes: title={'center': 'Length'}&gt;,\n        &lt;Axes: title={'center': 'Diameter'}&gt;,\n        &lt;Axes: title={'center': 'Height'}&gt;],\n       [&lt;Axes: title={'center': 'Whole weight'}&gt;,\n        &lt;Axes: title={'center': 'Shucked weight'}&gt;,\n        &lt;Axes: title={'center': 'Viscera weight'}&gt;],\n       [&lt;Axes: title={'center': 'Shell weight'}&gt;,\n        &lt;Axes: title={'center': 'Age'}&gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n(a) Distribution ofnumerical features.\n\n\n\n\n\n\n\n\n\n\n(b)\n\n\n\n\n\n\nFigure 15\n\n\n\n\nFrom Figure 15, it is observed that the numerical features are highly skewed and so skewness should be found and apply some normalization. Skewness of these numerical features can be found as follows.\n\n\nCode\n# finding the measure of skewness for numerical features\ntemp = pd.concat([df['Length'], df['Diameter'],df['Height'],df['Whole weight'],df['Shucked weight'],df['Viscera weight'],df['Shell weight']], axis=1)\ntemp.skew().sort_values(ascending = False)\n\n\nHeight            3.166364\nShucked weight    0.718735\nShell weight      0.621081\nViscera weight    0.591455\nWhole weight      0.530549\nDiameter         -0.610182\nLength           -0.640993\ndtype: float64\n\n\nFrom the skewness measures, it is found that the feature Height is highly positively skewed. So it should be normalized using some suitable transformation. Since all the height measures are positive, square root transformation is a better choice. Further we are building a predictive model based on measurable, non-lethal features, the subcomponent weights may be omitted. A final discision will be taken based on correlation analysis.\n\nCorrelation Analysis\n\nCorrelation measures the strength and direction of the linear relationship between independent variables (features). A correlation matrix is a common tool for examining relationships between multiple variables.\n\n\nCode\ncorr = temp.corr()\nplt.figure(figsize = (8,8))\nax = sns.heatmap(corr, vmin = -1, center = 0, annot = True, cmap = 'mako')\n\n\n\n\n\n\n\n\nFigure 16: Correlation matrix of numerical features\n\n\n\n\n\nFigure 16 shows the correlation matrix of the numerical features in the dataset. As expected, the subcomponents of weight of abalone shows very high correlation with the feature Whole weight, we can drop the subcomponents whithout loosing feature information. Also high coorelation found between the features, Length & Diameter. So let’s find those features which shows similar pattern and to be excluded.\n\n\nCode\nupper_tri = corr.where(np.triu(np.ones(corr.shape),k=1).astype(bool))\ncolumns_to_drop = [column for column in upper_tri.columns if any(upper_tri[column] &gt; 0.95)] #highly correlated variables to be removed.\nprint(\"Columns to drop:\\n\", columns_to_drop)\n\n\nColumns to drop:\n ['Diameter', 'Shucked weight', 'Viscera weight', 'Shell weight']\n\n\n\nSelection of skillful features\n\nFrom the correlation analysis, Diameter, Shuckle weight, Viscera weight and Shell weights are redundant features. So they are dropped and further the feature Height will be normalized with square root transformation.\n\n\nCode\ndf.drop(columns_to_drop, axis=1, inplace = True)\n# apply square root trasnformation\ndf['Height'] = np.sqrt(df['Height'])\n\n\n\n\nCode\n# rechecking skewness\ndf.skew().sort_values(ascending = False)\n\n\nAge             1.113754\nWhole weight    0.530549\nHeight         -0.176012\nLength         -0.640993\ndtype: float64\n\n\nNow all the independent variables are near to zero skewness. Following histogram verify this observation.\n\n\nCode\ndf.hist(figsize = (20,10), layout = (2,4), bins = 30)\n\n\narray([[&lt;Axes: title={'center': 'Length'}&gt;,\n        &lt;Axes: title={'center': 'Height'}&gt;,\n        &lt;Axes: title={'center': 'Whole weight'}&gt;,\n        &lt;Axes: title={'center': 'Age'}&gt;],\n       [&lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;, &lt;Axes: &gt;]], dtype=object)\n\n\n\n\n\n\n\n\n\n\nCreation of a new categorical variables using the variable Age\n\nBased on the age distribution of the Abalone in the given datset, a new categroical variable is created as follows: \\[\n\\text{Age Group}=\\begin{cases}1&;\\quad 2.5\\leq \\text{Age}&lt;9.5\\\\ 2&;\\quad 9.5\\leq \\text{Age}&lt;12.5\\\\3&; \\quad 12.5\\leq \\text{Age}\\leq 30.5\\end{cases}\n\\]\nNow the structure of the updated dataset will be as follows.\n\n\nCode\ndf['Age_Group'] = pd.cut(x=df['Age'], bins=[2.5,9.5,12.5,30.5],\n                     labels=['1', '2', '3'])\ndf.head()\n\n\n\n\n\n\n\n\n\nLength\nHeight\nWhole weight\nAge\nAge_Group\n\n\n\n\n0\n0.455\n0.308221\n0.5140\n16.5\n3\n\n\n1\n0.350\n0.300000\n0.2255\n8.5\n1\n\n\n2\n0.530\n0.367423\n0.6770\n10.5\n2\n\n\n3\n0.440\n0.353553\n0.5160\n11.5\n2\n\n\n4\n0.330\n0.282843\n0.2050\n8.5\n1\n\n\n\n\n\n\n\nDistribution of samples over the newly created categorical variable is shown in Figure 17.\n\n\nCode\nsns.countplot(x='Age_Group', data=df)\n#plt.title('Distributed Classes')\nplt.show()\n\n\n\n\n\n\n\n\nFigure 17: Sample distribution over Age_Group.\n\n\n\n\n\n\nMLP models for classification\n\nNow the final data for MLP model can be created as follows.\n\n\nCode\n# creating dataset for classification\n#Length Height  Whole weight\ndf_c = pd.concat([df['Length'], df['Height'],df['Height'],df['Whole weight'],df['Age_Group']], axis=1)\n# dataset for regression\n\ndf_r = pd.concat([df['Length'], df['Height'],df['Height'],df['Whole weight'],df['Age']], axis=1)\n\n\n\n\nCode\n# prepare input and target from the engineered dataset\nX=df_c.drop('Age_Group', axis = 1)\n#X = df.drop('Age_Group', axis = 1)\ny = df['Age_Group']\n\n\n\nBulding MLP model\n\nIn this section we will build an MLP model with two hidden layers for the classification job. Python code for this task is given below.\n\n\nCode\n# loading libraries: classification mlp model for the abalone dataset\nfrom numpy import unique\nfrom numpy import argmax\nfrom pandas import read_csv\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\n\n\n\nCode\nX, y = X.astype('float'), y.astype('float')\nn_features = X.shape[1]\n# encode strings to integer\ny = LabelEncoder().fit_transform(y)\nn_class = len(unique(y))\n# split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1)\n# define the keras model\nmodel = Sequential()\nmodel.add(Dense(20, input_dim=n_features, activation='relu', kernel_initializer='he_normal'))\nmodel.add(Dense(10, activation='relu', kernel_initializer='he_normal'))\nmodel.add(Dense(n_class, activation='softmax'))\n# compile the keras model\nmodel.compile(loss='sparse_categorical_crossentropy', optimizer='adam',metrics=['accuracy'])\n# fit the keras model on the dataset\nhistory=model.fit(X_train, y_train, epochs=150, batch_size=32,validation_split=0.2, verbose=False)\n\n\nModel loss and accuracy during the training and the validation is shown in ?@fig-Abal.\n\n\nCode\n# Plot accuracy and loss from history\nplt.figure(figsize=(8, 5))\n\n# Accuracy plot\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Train Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend()\n\n# Loss plot\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Train Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.title('Model Loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nFigure 18: Model loss and accuracy during training and validation.\n\n\n\n\n\n\nPerformance Evaluation\n\nSkill of the MLP classification model that we built is evaluated using the measures- confusion matrix, accuracy, precision, recall, f1 score. Class-wise accuracy will be assessed using the classification report. All these measures are evaluated using the following Python code.\n\n\nCode\nfrom sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score,classification_report\nyhat = model.predict(X_test)\n# Calculate confusion matrix\nyhat_class = np.argmax(yhat, axis=1)  # Predicted classes\ncm = confusion_matrix(y_test, yhat_class)\n# Calculate classification metrics\naccuracy = accuracy_score(y_test, yhat_class)\nprecision = precision_score(y_test, yhat_class, average='weighted')  # Weighted for multiclass\nrecall = recall_score(y_test, yhat_class, average='weighted')\nf1 = f1_score(y_test, yhat_class, average='weighted')\n\n# Display metrics\nprint(\"Confusion Matrix:\")\nprint(cm)\nprint(f\"Accuracy: {accuracy:.3f}\")\nprint(f\"Precision: {precision:.3f}\")\nprint(f\"Recall: {recall:.3f}\")\nprint(f\"F1 Score: {f1:.3f}\")\nprint(classification_report(y_test, yhat_class))\n\n\n 1/44 ━━━━━━━━━━━━━━━━━━━━ 2s 54ms/step\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 1ms/step \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b44/44 ━━━━━━━━━━━━━━━━━━━━ 0s 2ms/step\nConfusion Matrix:\n[[352  89  10]\n [131 440  45]\n [ 37 216  58]]\nAccuracy: 0.617\nPrecision: 0.601\nRecall: 0.617\nF1 Score: 0.588\n              precision    recall  f1-score   support\n\n           0       0.68      0.78      0.73       451\n           1       0.59      0.71      0.65       616\n           2       0.51      0.19      0.27       311\n\n    accuracy                           0.62      1378\n   macro avg       0.59      0.56      0.55      1378\nweighted avg       0.60      0.62      0.59      1378\n\n\n\n\nKey Metrics:\n\n\nAccuracy: 0.622\nThe model correctly classified 62.2% of the total samples. This indicates room for improvement in overall performance.\nPrecision: 0.610\nPrecision reflects the proportion of true positive predictions among all positive predictions. A precision of 0.610 suggests a moderate rate of correct positive predictions.\nRecall: 0.622\nRecall measures the proportion of true positives that were correctly identified. A recall of 0.622 shows the model detects positive samples moderately well.\nF1 Score: 0.587\nThe F1 Score, balancing precision and recall, indicates the model struggles to maintain consistency across all metrics.\n\n\nClass-wise Metrics:\n\n\n\n\nClass\nPrecision\nRecall\nF1-Score\nSupport\n\n\n\n\n0\n0.68\n0.78\n0.72\n451\n\n\n1\n0.59\n0.74\n0.66\n616\n\n\n2\n0.54\n0.16\n0.24\n311\n\n\n\n\nClass 0: Performs relatively well with good precision and recall.\nClass 1: Moderate performance but better recall indicates more correct detections for this class.\nClass 2: Poor performance with low recall (16%), indicating the model struggles to detect this class.\n\n\nOverall Metrics:\n\n\n\n\nMetric\nValue\n\n\n\n\nAccuracy\n0.622\n\n\nMacro Avg\n0.610\n\n\nWeighted Avg\n0.610\n\n\n\n\nMacro Average: Unweighted average across classes, showing a slightly lower overall performance.\nWeighted Average: Reflects class imbalance, weighting metrics by the number of samples in each class.\n\n\nFindings:\n\n\nClass Imbalance: Class 2 shows significantly lower recall and F1-score, indicating difficulty in detecting samples from this class.\nMisclassifications:\n\nMany Class 2 samples are misclassified as Class 1 (223 instances).\nClass 0 performs better but still has notable misclassifications into Class 1.\n\n\n\nConclusion:The model performs moderately well overall but struggles with:\n\n\nDetecting Class 2 effectively.\nManaging misclassifications between Class 1 and Class 2.\n\nImproving the dataset balance, refining the model, or employing techniques like class-specific weighting or advanced algorithms could enhance the performance."
  },
  {
    "objectID": "assignments.html",
    "href": "assignments.html",
    "title": "Assignments(DL)",
    "section": "",
    "text": "The assignments documented here reflect a comprehensive study of the concepts and tools taught in the course Deep Learning. These include:\n\n\n\nANN Architecture\n\nDesigning ANN using Python OOP concepts\nDevelop funtions to simulate ANN models with required methods for summarizing parameter details.\nDesign a user friendly framework for ANN model creation like Keras\n\nPopular Activation functions and their properties\n\nDefinition and mathematical properties.\nVisualization.\nComparitive study in the context of Deep Learning.\n\nMulti Layer Perceptron\n\nMLP for regression task.\nMLP for classification task.\n\n\n\nFor convenience and continuity, each set of assignments is embedded for easy access:\n\n\n\nThis assignment contains."
  },
  {
    "objectID": "assignments.html#introduction",
    "href": "assignments.html#introduction",
    "title": "Assignments(DL)",
    "section": "",
    "text": "The assignments documented here reflect a comprehensive study of the concepts and tools taught in the course Deep Learning. These include:\n\n\n\nANN Architecture\n\nDesigning ANN using Python OOP concepts\nDevelop funtions to simulate ANN models with required methods for summarizing parameter details.\nDesign a user friendly framework for ANN model creation like Keras\n\nPopular Activation functions and their properties\n\nDefinition and mathematical properties.\nVisualization.\nComparitive study in the context of Deep Learning.\n\nMulti Layer Perceptron\n\nMLP for regression task.\nMLP for classification task.\n\n\n\nFor convenience and continuity, each set of assignments is embedded for easy access:\n\n\n\nThis assignment contains."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Welcome to my academic portfolio! I am a researcher scholar and student specializing in Deep Learning and Artificial Intelligence. This website serves as a comprehensive repository of my projects, assignments, course materials, and other related works from my research coursework."
  },
  {
    "objectID": "about.html#about-me",
    "href": "about.html#about-me",
    "title": "About",
    "section": "",
    "text": "Welcome to my academic portfolio! I am a researcher scholar and student specializing in Deep Learning and Artificial Intelligence. This website serves as a comprehensive repository of my projects, assignments, course materials, and other related works from my research coursework."
  },
  {
    "objectID": "about.html#academic-background",
    "href": "about.html#academic-background",
    "title": "About",
    "section": "2 Academic Background",
    "text": "2 Academic Background\nI am currently pursuing my P.hD in Artificial Intelligence at Amrita Vishwa Vidyapeetham. My research focuses on Automating Image Denoising Algorithm Parameters. I have a strong foundation in computational mathematics,machine learning, neural networks, and data analysis, which I have applied to various projects and assignments documented here."
  },
  {
    "objectID": "about.html#research-interests",
    "href": "about.html#research-interests",
    "title": "About",
    "section": "3 Research Interests",
    "text": "3 Research Interests\n\nDeep Learning\nArtificial Intelligence\nNeural Networks\nMachine Learning\nData Science"
  },
  {
    "objectID": "about.html#projects-and-assignments",
    "href": "about.html#projects-and-assignments",
    "title": "About",
    "section": "4 Projects and Assignments",
    "text": "4 Projects and Assignments\nThis portfolio includes detailed documentation of my projects and assignments, showcasing my skills and knowledge in designing and implementing advanced machine learning models."
  },
  {
    "objectID": "about.html#contact",
    "href": "about.html#contact",
    "title": "About",
    "section": "5 Contact",
    "text": "5 Contact\nFeel free to reach out to me for any academic or professional inquiries. You can contact me at [your email address] or connect with me on sijuswamy.research@gmail.com.\nThank you for visiting my portfolio!"
  },
  {
    "objectID": "assignments_old.html",
    "href": "assignments_old.html",
    "title": "Assignments(CMD)",
    "section": "",
    "text": "The assignments documented here reflect a comprehensive study of the concepts and tools taught in the course Computational Mathematics for Data Science. These include:\n\n\n\nLinear Algebra Fundamentals:\n\nMatrix operations, four fundamental subspaces, Matrix decompositions- CR, LU, QR, Spectral , SVD and PCA.\nSolving systems of linear equations.\nApplications in Machine Learning.\n\nAdvanced Topics in Optimization:\n\nConvex optimization and its applications.\nLagrange methods for optimization.\nApplications in computational data science."
  },
  {
    "objectID": "assignments_old.html#introduction",
    "href": "assignments_old.html#introduction",
    "title": "Assignments(CMD)",
    "section": "",
    "text": "The assignments documented here reflect a comprehensive study of the concepts and tools taught in the course Computational Mathematics for Data Science. These include:\n\n\n\nLinear Algebra Fundamentals:\n\nMatrix operations, four fundamental subspaces, Matrix decompositions- CR, LU, QR, Spectral , SVD and PCA.\nSolving systems of linear equations.\nApplications in Machine Learning.\n\nAdvanced Topics in Optimization:\n\nConvex optimization and its applications.\nLagrange methods for optimization.\nApplications in computational data science."
  },
  {
    "objectID": "assignments_old.html#structure",
    "href": "assignments_old.html#structure",
    "title": "Assignments(CMD)",
    "section": "2 Structure",
    "text": "2 Structure\nEach of the 83 assignments follows a consistent structure:\n\nConceptual Summary: A concise review of the topic’s theoretical framework.\nProblem Solving: Mathematical derivations and computational implementation using MATLAB.\nOptimization Techniques: For relevant topics, problems were solved using the CVX solver.\n\n\nFor convenience and continuity, each set of assignments is embedded for easy access:\n\n2.1 Assignment Set 1\nThis set contains assignment 1 to 10.\n\n\n\n\n\n2.2 Assignment Set 2\nThis set contains assignment 11 to 20.\n\n\n\n\n\n2.3 Assignment Set 3\nThis set contains assignment 21 to 30.\n\n\n\n\n\n2.4 Assignment Set 4\nThis set contains assignment 31 to 48.\n\n\n\n\n\n2.5 Assignment Set 5\nThis set contains assignment 49 to 64.\n\n\n\n\n\n2.6 Assignment Set 6\nThis set contains assignment 65 to 79.\n\n\n\n\n\n2.7 Assignment Set 7\nThis set contains assignment 80 to 83."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Academic Portfolio",
    "section": "",
    "text": "Welcome to my academic portfolio! This site is dedicated to showcasing my journey and achievements in the realm of Deep Learning and Artificial Intelligence. Here, you will find a curated collection of my projects, assignments, and course materials that highlight my expertise and passion for this field. Whether you are a fellow researcher, a potential collaborator, or simply interested in my work, I invite you to explore and engage with the content. Thank you for visiting!"
  }
]